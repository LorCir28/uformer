{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y_wpJixyu0uG",
        "5KVlRJ0lu7eU",
        "smh0pVrvtHTZ",
        "JXRFscaCxaY7",
        "maczFOi8eB52",
        "4rzLgWkBY50K",
        "lV0TTJ-b34SV"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "y_wpJixyu0uG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtF7IK-nuqlb",
        "outputId": "b3dcdc44-5d13-4512-969e-233c8865dd24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_var = {\n",
        "    # Resolutions\n",
        "    'RGB_img_res': (3, 64, 16),\n",
        "\n",
        "    # Parameters\n",
        "    'batch_size': 8,\n",
        "    'n_workers': 2,\n",
        "    'seed': 10000,\n",
        "    'lr': 1e-3,\n",
        "    'lr_patience': 15,\n",
        "    'epochs': 10,\n",
        "    'n_workers': 2,\n",
        "    'e_stop_epochs': 30,\n",
        "\n",
        "    # Operations\n",
        "    'do_print_model': True\n",
        "}\n",
        "\n",
        "augmentation_parameters = {\n",
        "    # TODO\n",
        "}"
      ],
      "metadata": {
        "id": "kEN7u0ovvOaU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root = '/content/drive/MyDrive/NN_project/SSID_dataset/'\n",
        "save_model_root = '/content/drive/MyDrive/NN_project/'\n",
        "model_name = \"Uformer\""
      ],
      "metadata": {
        "id": "0hQKBwXMv3yI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5KVlRJ0lu7eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops torchsummaryX --quiet"
      ],
      "metadata": {
        "id": "tJ2A1RHB2KW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cc0548-0a4a-4283-b98b-64562db9da2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TT\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchsummaryX import summary\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "x-ZMoEfcu9gM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "smh0pVrvtHTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
        "  valid_mask = y_true > 0.0\n",
        "  valid_pred = y_pred[valid_mask]\n",
        "  valid_true = y_true[valid_mask]\n",
        "  correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
        "  return 100 * torch.mean(correct.float())\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "      return param_group['lr']\n",
        "\n",
        "def hardware_check():\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(\"Actual device: \", device)\n",
        "  return device\n",
        "\n",
        "def load_pretrained_model(model, device):\n",
        "  print(\"Loading checkpoint...\\n\")\n",
        "  model_dict = torch.load(save_checkpoint, map_location=torch.device(device))\n",
        "  model.load_state_dict(model_dict)\n",
        "  print(\"Checkpoint loaded!\\n\")\n",
        "\n",
        "def plot_graph(f, g, f_label, g_label, title, path):\n",
        "  epochs = range(0, len(f))\n",
        "  plt.plot(epochs, f, 'b', label=f_label)\n",
        "  plt.plot(epochs, g, 'orange', label=g_label)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(path + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def plot_history(history, path):\n",
        "  plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
        "  plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
        "\n",
        "def plot_loss(history,title):\n",
        "  l_train_list = history['train_loss']\n",
        "  l_test_list = history['val_loss']\n",
        "  epochs = range(0, len(l_train_list))\n",
        "\n",
        "  plt.plot(epochs, l_train_list, 'r', label='Train loss')\n",
        "  plt.plot(epochs, l_test_list, 'g', label='Test loss')\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(save_model_root + \"/\" + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def print_model(model, device, input_shape):\n",
        "  info = summary(model, torch.ones((global_var['batch_size'], input_shape[0], input_shape[1], input_shape[2])).to(device))\n",
        "  info.to_csv(save_model_root + 'model_summary.csv')\n",
        "\n",
        "def save_checkpoint(model, name):\n",
        "  torch.save(model.state_dict(), save_model_root + name)\n",
        "\n",
        "def save_csv_history(model_name, path):\n",
        "  objects = []\n",
        "  with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
        "      while True:\n",
        "          try:\n",
        "              objects.append(pickle.load(openfile))\n",
        "          except EOFError:\n",
        "              break\n",
        "  df = pd.DataFrame(objects)\n",
        "  df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
        "\n",
        "def save_history(history, filepath):\n",
        "  tmp_file = open(filepath + '.pkl', \"wb\")\n",
        "  pickle.dump(history, tmp_file)\n",
        "  tmp_file.close()"
      ],
      "metadata": {
        "id": "ZlZooBR0tO_D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "JXRFscaCxaY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation"
      ],
      "metadata": {
        "id": "oShoEPlgyVxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "qMmArppAyYRS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "KhjFnJ-yyaCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SSID_Dataset(Dataset):\n",
        "    def __init__(self, data_root):\n",
        "        self.dataset_path = data_root\n",
        "        self.dir_list = data_root + \"Scene_Instances.txt\"\n",
        "        self.data_dir = data_root + \"Data/\"\n",
        "        self.data_directiories = []\n",
        "        self.img_paths = []\n",
        "        self.target_paths = []\n",
        "        self.post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]),antialias=None),\n",
        "        ])\n",
        "\n",
        "        data_dir_file = open(dataset_root+\"Scene_Instances.txt\", 'r')\n",
        "        self.data_directories = [elem.strip() for elem in data_dir_file.readlines()]\n",
        "        data_dir_file.close()\n",
        "\n",
        "        for elem in self.data_directories:\n",
        "          data_path = self.data_dir + elem\n",
        "          content = sorted(os.listdir(data_path))\n",
        "          self.target_paths.append(content[0])\n",
        "          self.img_paths.append(content[1])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.data_dir + self.data_directories[index] + \"/\" + self.img_paths[index]\n",
        "        img = self.post_processing(Image.open(img_path))\n",
        "\n",
        "        target_path = self.data_dir + self.data_directories[index] + \"/\" + self.target_paths[index]\n",
        "        target = self.post_processing(Image.open(target_path))\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ],
      "metadata": {
        "id": "LroEZr8QycjJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "OyizoOj4yehn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SSID_Dataset(dataset_root)\n",
        "train_dataset, test_dataset = random_split(dataset, [112, 48])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size = global_var['batch_size'],\n",
        "                          num_workers = global_var['n_workers'],\n",
        "                          shuffle = True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = global_var['batch_size'],\n",
        "                         num_workers = global_var['n_workers'],\n",
        "                         shuffle = True)\n",
        "\n",
        "print(\"Train data percentage: \", len(train_dataset)/(len(train_dataset)+len(test_dataset)))\n",
        "print(\"Test data percentage: \", len(test_dataset)/(len(train_dataset)+len(test_dataset)))"
      ],
      "metadata": {
        "id": "NxVEwFxtyf92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680cd8f5-8f1e-4c97-c22d-166721f74c4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data percentage:  0.7\n",
            "Test data percentage:  0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "wr4UgpsTQNXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class loss_function(nn.Module):\n",
        "#   def __init__(self, epsilon=1e-3):\n",
        "#     super(loss_function,self).__init__()\n",
        "#     self.epsilon = epsilon\n",
        "\n",
        "#   def forward(self,pred,truth):\n",
        "#     return torch.mean(torch.sqrt((pred-truth)**2 + self.epsilon**2))\n",
        "\n",
        "class loss_function(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(loss_function, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "UGzJ-r8QQO1V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics"
      ],
      "metadata": {
        "id": "maczFOi8eB52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# It works, compared with torchmetrics.image import PeakSignalNoiseRatio\n",
        "def psnr(original_img, compressed_img, max_pix_val=1.0):\n",
        "  mse = torch.mean((original_img-compressed_img)**2)\n",
        "  return 20 * torch.log10(max_pix_val/torch.sqrt(mse))\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# ATTENTION: 4D tensors needed\n",
        "# It works, compared with StructuralSimilarityIndexMeasure from torchmetrics.image\n",
        "def ssim(original_img, restored_img, max_pix_val=1.0, window_size=11, window=None, size_average=True, full=False):\n",
        "    (_, channel, height, width) = original_img.size()\n",
        "    real_size = min(window_size, height, width)\n",
        "    window = create_window(real_size, channel=channel).to(original_img.device)\n",
        "\n",
        "    mu1 = F.conv2d(original_img, window, padding=0, groups=channel)\n",
        "    mu2 = F.conv2d(restored_img, window, padding=0, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(original_img ** 2, window, padding=0, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(restored_img ** 2, window, padding=0, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(original_img * restored_img, window, padding=0, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * max_pix_val) ** 2\n",
        "    C2 = (0.03 * max_pix_val) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "\n",
        "    return (((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)).mean()\n",
        "\n",
        "\n",
        "def compute_evaluation(test_dataloader, model, device='cpu'):\n",
        "  model.eval()\n",
        "  psnr_values = []\n",
        "  ssim_values = []\n",
        "\n",
        "  for _, (inputs, targets) in enumerate(test_dataloader):\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          predictions = model(inputs)\n",
        "\n",
        "      psnr_values.append(psnr(targets,predictions))\n",
        "      ssim_values.append(ssim(targets,predictions))\n",
        "\n",
        "  return torch.mean(torch.Tensor(psnr_values)).item(),torch.mean(torch.Tensor(ssim_values)).item()"
      ],
      "metadata": {
        "id": "Wm4POhkmeDsJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "4rzLgWkBY50K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention components\n",
        "# Attention module\n",
        "class W_MSA(nn.Module):\n",
        "  def __init__(self, dim=32, num_heads=8, qkv_bias=False):\n",
        "    super(W_MSA, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = dim // num_heads\n",
        "\n",
        "    # nn.Linear(in_features, out_features): the input of the layer has to have the last dimension equal to in_features (namely (*, in_features)). The output of the layer has the\n",
        "    # same dimension of the input except for the last one which is equal to out_features (namely (*, out_features))\n",
        "\n",
        "    # self.qkv = nn.Linear(dim, num_heads, self.head_dim, bias=qkv_bias) # this layer returns the queries, keys and values\n",
        "    self.qkv = nn.Linear(dim, self.head_dim*self.num_heads*3, bias=qkv_bias)\n",
        "\n",
        "    # these are default layers for the attention module\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(0.)\n",
        "    self.attn_drop = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    # print(\"********* x shape: \",x.shape)\n",
        "    # print(\"********* self qkv shape:\",self.qkv(x).shape)\n",
        "\n",
        "    qkv_temp = self.qkv(x)\n",
        "    mult = qkv_temp.shape[0] * qkv_temp.shape[1] * qkv_temp.shape[2]\n",
        "    mult = mult//3\n",
        "    mult = mult//(self.num_heads*4)\n",
        "    mult = mult//global_var['batch_size']\n",
        "\n",
        "    qkv = qkv_temp.reshape(B, mult, self.num_heads*4, 3)\n",
        "    # print(\"********* qkv shape:\",qkv.shape)\n",
        "    q, k, v = qkv.unbind(dim=-1) # this returnes a tuple of tensors whose each element is portion of the original tensor (qkv) (ref: https://pytorch.org/docs/stable/generated/torch.unbind.html)\n",
        "\n",
        "    # this is the implementation of the attention formula described on the paper\n",
        "    scale = (C // self.head_dim) ** (0.5)\n",
        "    attn = ((q @ k.transpose(-2, -1)) // scale) + B # from the github: the final B can be also removed\n",
        "    attn = attn.softmax(dim=1)\n",
        "    attn = self.attn_drop(attn)\n",
        "    # print(\"********** x no reshape:\",(attn @ v).transpose(1, 2).shape)\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "    # these are default for the attention module\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# this is the simple implementation described in the paper\n",
        "class LeFF(nn.Module):\n",
        "  def __init__(self, dim=32, hidden_dim=128):\n",
        "    super(LeFF, self).__init__()\n",
        "    self.dim = dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.layer1 = nn.Sequential(nn.Linear(dim, hidden_dim), nn.GELU())\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1), nn.GELU())\n",
        "    self.layer3 = nn.Sequential(nn.Linear(hidden_dim, dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(\"Before 1st layer x: \", x.shape)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x.permute(2,1,0))\n",
        "    x = self.layer3(x.permute(2,1,0))\n",
        "\n",
        "    return x\n",
        "\n",
        "  # def forward(self, x):\n",
        "  #   # bs x hw x c\n",
        "  #   bs, hw, c = x.size()\n",
        "  #   hh = int(math.sqrt(hw))\n",
        "\n",
        "  #   x = self.layer1(x)\n",
        "\n",
        "  #   # spatial restore\n",
        "  #   x = rearrange(x, ' b (h w) (c) -> b c h w ', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer2(x)\n",
        "\n",
        "  #   # flatten\n",
        "  #   x = rearrange(x, ' b c h w -> b (h w) c', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer3(x)\n",
        "\n",
        "  #   return x\n",
        "\n",
        "# NN BLOCKS\n",
        "# LeWin Transformer Block (from the paper, it is made up of a sequence of: NormLayer, W_MSA, NormLayer, LeFF)\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.w_msa = W_MSA(dim=dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.leff = LeFF(dim=dim)\n",
        "    self.dropout = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(self.norm1(x))\n",
        "    x = self.w_msa(x)\n",
        "    x = self.dropout(self.norm2(x))\n",
        "    x = self.leff(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Down-sampling Block (reduces the size of the feature map)\n",
        "# reshape the flattened features into 2D spatial feature maps, and then down-sample the maps, double the channels using 4 × 4 convolution with stride 2\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DownsampleBlock, self).__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # remember that x is a tensor!!\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        # print(\"*************** x: \",x.shape)\n",
        "        x = x.transpose(1, 2).contiguous().view(B, C, H, W) # this transposes the 1st and 2nd dimension of x, then the size of x is reshaped with view (the new size is (B, C, H, W))\n",
        "                                                            # (.contiguous() is required to make view workable, since view works only on contiguous data)\n",
        "\n",
        "        out = self.conv(x).flatten(2).transpose(1, 2).contiguous() # this pass the input x to the downsample layer, then the 2nd dimension of the output is flattened with the 3rd\n",
        "                                                                   # and finally its 1st and 2nd dimensions are transposed\n",
        "\n",
        "                                                                   # (B, C, H*W) is the size of the out after flatten(2)\n",
        "                                                                   # (B H*W C) is the final size of the out after transpose(1, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Up-sampling Block (reduces half of the channels and doubles the size of the feature map)\n",
        "# 2 × 2 transposed convolution with stride 2\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding):\n",
        "      super(UpsampleBlock, self).__init__()\n",
        "      self.in_channel = in_channel\n",
        "      self.out_channel = out_channel\n",
        "      self.deconv = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,padding=padding),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      B, L, C = x.shape\n",
        "      H = int(math.sqrt(L))\n",
        "      W = int(math.sqrt(L))\n",
        "      x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "      out = self.deconv(x).flatten(2).transpose(1, 2).contiguous() # B H*W C\n",
        "\n",
        "      return out\n",
        "\n",
        "# Input Projection Block (extracts the low-level features)\n",
        "# 3 x 3 convolutional layer with LeakyReLu\n",
        "class InputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=3, out_channel=32, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C\n",
        "\n",
        "        return x\n",
        "\n",
        "# Output Projection Block (returns the residual)\n",
        "# 3 x 3 convolutional layer\n",
        "class OutputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=64, out_channel=3, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x # the output of this block si called residual\n",
        "\n",
        "# complete uformer class\n",
        "class Uformer(nn.Module):\n",
        "  def __init__(self, embed_dim=32):\n",
        "    super().__init__()\n",
        "\n",
        "    # encoder\n",
        "    self.input_proj = InputProjBlock()\n",
        "\n",
        "    self.tranformerblock_0 = TransformerBlock(embed_dim)\n",
        "    self.downsample_0 = DownsampleBlock(embed_dim, embed_dim*2)\n",
        "\n",
        "    self.tranformerblock_1 = TransformerBlock(embed_dim*2)\n",
        "    self.downsample_1 = DownsampleBlock(embed_dim*2, embed_dim*4)\n",
        "\n",
        "    self.tranformerblock_2 = TransformerBlock(embed_dim*4)\n",
        "    self.downsample_2 = DownsampleBlock(embed_dim*4, embed_dim*8)\n",
        "\n",
        "    self.tranformerblock_3 = TransformerBlock(embed_dim*8)\n",
        "    self.downsample_3 = DownsampleBlock(embed_dim*8, embed_dim*16)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    self.tranformerblock_4 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    self.upsample_0 = UpsampleBlock(embed_dim*16, embed_dim*8,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_5 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "    self.upsample_1 = UpsampleBlock(embed_dim*16, embed_dim*4,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_6 = TransformerBlock(embed_dim*8)\n",
        "\n",
        "    self.upsample_2 = UpsampleBlock(embed_dim*8, embed_dim*2,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_7 = TransformerBlock(embed_dim*4)\n",
        "\n",
        "    self.upsample_3 = UpsampleBlock(embed_dim*4, embed_dim,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_8 = TransformerBlock(embed_dim*2)\n",
        "\n",
        "    self.output_proj = OutputProjBlock()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    degraded_image = x # x is the degraded image\n",
        "\n",
        "\n",
        "    # encoder\n",
        "    y = self.input_proj(x)\n",
        "    t0 = self.tranformerblock_0(y)\n",
        "    d0 = self.downsample_0(t0)\n",
        "    t1 = self.tranformerblock_1(d0)\n",
        "    d1 = self.downsample_1(t1)\n",
        "    t2 = self.tranformerblock_2(d1)\n",
        "    d2 = self.downsample_2(t2)\n",
        "    t3 = self.tranformerblock_3(d2)\n",
        "    d3 = self.downsample_3(t3)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    t4 = self.tranformerblock_4(d3)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    u0 = self.upsample_0(t4)\n",
        "    # print(\"1) Upsampled in: \",u0.shape)\n",
        "    # print(\"1) Skipped in: \",t3.shape)\n",
        "    skippedconn_0 = torch.cat([u0, t3], -1) # this creates a skipped connection between t3 and t6 (u0 would have to be the input of t5)\n",
        "    t5 = self.tranformerblock_5(skippedconn_0)\n",
        "\n",
        "    u1 = self.upsample_1(t5)\n",
        "    # print(\"2) Upsampled in: \",u1.shape)\n",
        "    # print(\"2) Skipped in: \",t2.shape)\n",
        "    skippedconn_1 = torch.cat([u1, t2], -1)\n",
        "    t6 = self.tranformerblock_6(skippedconn_1)\n",
        "\n",
        "    u2 = self.upsample_2(t6)\n",
        "    # print(\"3) Upsampled in: \",u2.shape)\n",
        "    # print(\"3) Skipped in: \",t1.shape)\n",
        "    skippedconn_2 = torch.cat([u2, t1], -1)\n",
        "    t7 = self.tranformerblock_7(skippedconn_2)\n",
        "\n",
        "    u3 = self.upsample_3(t7)\n",
        "    # print(\"4) Upsampled in: \",u3.shape)\n",
        "    # print(\"4) Skipped in: \",t0.shape)\n",
        "    skippedconn_3 = torch.cat([u3, t0], -1)\n",
        "    t8 = self.tranformerblock_8(skippedconn_3)\n",
        "\n",
        "    residual = self.output_proj(t8)\n",
        "\n",
        "\n",
        "    # final residual summation\n",
        "    # print(\"Degraded: \",degraded_image.shape)\n",
        "    # print(\"Residual: \",residual.shape)\n",
        "    restored_image = degraded_image + residual.reshape([residual.shape[0],residual.shape[1],degraded_image.shape[2],degraded_image.shape[3]])\n",
        "\n",
        "    return restored_image"
      ],
      "metadata": {
        "id": "Btr_ivEiY73R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "5hCfDRBKpV_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5a17yBPGw8o",
        "outputId": "bcd505a6-dee4-46b7-c969-83479174f3a0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(device,train_dataloader,test_dataloader):\n",
        "  # Set-seed\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "  np.random.seed(global_var['seed'])\n",
        "  torch.cuda.manual_seed(global_var['seed'])\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Globals\n",
        "  history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': []}\n",
        "  min_acc = 0\n",
        "  train_acc_list = []\n",
        "  test_acc_list = []\n",
        "  train_loss_list = []\n",
        "  test_loss_list = []\n",
        "\n",
        "  # Loss\n",
        "  criterion = loss_function()\n",
        "  from timm.utils import NativeScaler\n",
        "  loss_scaler = NativeScaler()\n",
        "\n",
        "  # Model\n",
        "  model = Uformer()\n",
        "  model.to(device=device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=global_var['RGB_img_res'])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=global_var['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
        "  )\n",
        "\n",
        "  # Scheduler\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=global_var['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
        "    cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
        "  )\n",
        "\n",
        "  # Early stopping\n",
        "  trigger_times, early_stopping_epochs = 0, global_var['e_stop_epochs']\n",
        "\n",
        "  print(\"--- Start training: {} ---\\n\".format(model_name))\n",
        "  # Train\n",
        "\n",
        "  for epoch in range(global_var['epochs']):\n",
        "    iter = 1\n",
        "    model.train()\n",
        "    running_loss, accuracy = 0, 0\n",
        "\n",
        "    with tqdm(train_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Training\")\n",
        "\n",
        "        # Load data\n",
        "        inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "        # Forward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_scaler(loss, optimizer,parameters=model.parameters())\n",
        "        # Backward\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Evaluation and Stats\n",
        "        running_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "\n",
        "        accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "        tepoch.set_postfix({'Loss': running_loss / iter,\n",
        "                            'Acc': accuracy.item() / iter,\n",
        "                            'Lr': global_var['lr'] if not history['lrs'] else history['lrs'][-1]})\n",
        "        iter += 1\n",
        "\n",
        "    # Validation\n",
        "    iter = 1\n",
        "    model.eval()\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    with tqdm(test_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Validation\")\n",
        "        inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "        # Validation loop\n",
        "        with torch.no_grad():\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Evaluation metrics\n",
        "          test_accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "          # Loss\n",
        "          loss = criterion(outputs, targets)\n",
        "          test_loss += loss.item()\n",
        "          test_loss_list.append(loss.item())\n",
        "\n",
        "          tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter})\n",
        "          iter += 1\n",
        "\n",
        "        # Update history infos\n",
        "        history['lrs'].append(get_lr(optimizer))\n",
        "        history['train_loss'].append(running_loss / len(train_dataloader))\n",
        "        history['val_loss'].append(test_loss / len(test_dataloader))\n",
        "        history['train_acc'].append(accuracy.item() / len(train_dataloader))\n",
        "        history['val_acc'].append(test_accuracy.item() / len(test_dataloader))\n",
        "\n",
        "        # Update scheduler LR\n",
        "        scheduler.step(history['test_rmse'][-1])\n",
        "\n",
        "        # Save model by best ACCURACY\n",
        "        if min_acc <= (test_accuracy / len(test_dataloader)):\n",
        "          min_acc = test_accuracy / len(test_dataloader)\n",
        "          save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
        "          print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
        "\n",
        "          if trigger_times > 4:\n",
        "            trigger_times = trigger_times - 2\n",
        "            print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
        "\n",
        "\n",
        "        save_history(history, save_model_root + model_name + '_history')\n",
        "        # Empty CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if trigger_times == early_stopping_epochs:\n",
        "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
        "            break\n",
        "\n",
        "        # Save loss for graphs\n",
        "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
        "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
        "\n",
        "    print('--- Finished Training ---')\n",
        "\n",
        "    save_csv_history(model_name=model_name, path=save_model_root)\n",
        "    plot_history(history, path=save_model_root)\n",
        "    plot_loss(history, path=save_model_root, title='Loss Trend')"
      ],
      "metadata": {
        "id": "7IkSzCVHpZxp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = hardware_check()\n",
        "train(device,train_loader,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQvL4hihzUer",
        "outputId": "28827d4e-910d-47d7-8d9a-27abc09c17e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual device:  cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================================================================\n",
            "                                                   Kernel Shape  \\\n",
            "Layer                                                             \n",
            "0_input_proj.proj.Conv2d_0                        [3, 32, 3, 3]   \n",
            "1_input_proj.proj.LeakyReLU_1                                 -   \n",
            "2_tranformerblock_0.LayerNorm_norm1                        [32]   \n",
            "3_tranformerblock_0.Dropout_dropout                           -   \n",
            "4_tranformerblock_0.w_msa.Linear_qkv                   [32, 96]   \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop                   -   \n",
            "6_tranformerblock_0.w_msa.Linear_proj                  [32, 32]   \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop                   -   \n",
            "8_tranformerblock_0.LayerNorm_norm2                        [32]   \n",
            "9_tranformerblock_0.Dropout_dropout                           -   \n",
            "10_tranformerblock_0.leff.layer1.Linear_0             [32, 128]   \n",
            "11_tranformerblock_0.leff.layer1.GELU_1                       -   \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "13_tranformerblock_0.leff.layer2.GELU_1                       -   \n",
            "14_tranformerblock_0.leff.layer3.Linear_0             [128, 32]   \n",
            "15_downsample_0.conv.Conv2d_0                    [32, 64, 4, 4]   \n",
            "16_tranformerblock_1.LayerNorm_norm1                       [64]   \n",
            "17_tranformerblock_1.Dropout_dropout                          -   \n",
            "18_tranformerblock_1.w_msa.Linear_qkv                 [64, 192]   \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop                  -   \n",
            "20_tranformerblock_1.w_msa.Linear_proj                 [64, 64]   \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop                  -   \n",
            "22_tranformerblock_1.LayerNorm_norm2                       [64]   \n",
            "23_tranformerblock_1.Dropout_dropout                          -   \n",
            "24_tranformerblock_1.leff.layer1.Linear_0             [64, 128]   \n",
            "25_tranformerblock_1.leff.layer1.GELU_1                       -   \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "27_tranformerblock_1.leff.layer2.GELU_1                       -   \n",
            "28_tranformerblock_1.leff.layer3.Linear_0             [128, 64]   \n",
            "29_downsample_1.conv.Conv2d_0                   [64, 128, 4, 4]   \n",
            "30_tranformerblock_2.LayerNorm_norm1                      [128]   \n",
            "31_tranformerblock_2.Dropout_dropout                          -   \n",
            "32_tranformerblock_2.w_msa.Linear_qkv                [128, 384]   \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop                  -   \n",
            "34_tranformerblock_2.w_msa.Linear_proj               [128, 128]   \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop                  -   \n",
            "36_tranformerblock_2.LayerNorm_norm2                      [128]   \n",
            "37_tranformerblock_2.Dropout_dropout                          -   \n",
            "38_tranformerblock_2.leff.layer1.Linear_0            [128, 128]   \n",
            "39_tranformerblock_2.leff.layer1.GELU_1                       -   \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "41_tranformerblock_2.leff.layer2.GELU_1                       -   \n",
            "42_tranformerblock_2.leff.layer3.Linear_0            [128, 128]   \n",
            "43_downsample_2.conv.Conv2d_0                  [128, 256, 4, 4]   \n",
            "44_tranformerblock_3.LayerNorm_norm1                      [256]   \n",
            "45_tranformerblock_3.Dropout_dropout                          -   \n",
            "46_tranformerblock_3.w_msa.Linear_qkv                [256, 768]   \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop                  -   \n",
            "48_tranformerblock_3.w_msa.Linear_proj               [256, 256]   \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop                  -   \n",
            "50_tranformerblock_3.LayerNorm_norm2                      [256]   \n",
            "51_tranformerblock_3.Dropout_dropout                          -   \n",
            "52_tranformerblock_3.leff.layer1.Linear_0            [256, 128]   \n",
            "53_tranformerblock_3.leff.layer1.GELU_1                       -   \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "55_tranformerblock_3.leff.layer2.GELU_1                       -   \n",
            "56_tranformerblock_3.leff.layer3.Linear_0            [128, 256]   \n",
            "57_downsample_3.conv.Conv2d_0                  [256, 512, 4, 4]   \n",
            "58_tranformerblock_4.LayerNorm_norm1                      [512]   \n",
            "59_tranformerblock_4.Dropout_dropout                          -   \n",
            "60_tranformerblock_4.w_msa.Linear_qkv               [512, 1536]   \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop                  -   \n",
            "62_tranformerblock_4.w_msa.Linear_proj               [512, 512]   \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop                  -   \n",
            "64_tranformerblock_4.LayerNorm_norm2                      [512]   \n",
            "65_tranformerblock_4.Dropout_dropout                          -   \n",
            "66_tranformerblock_4.leff.layer1.Linear_0            [512, 128]   \n",
            "67_tranformerblock_4.leff.layer1.GELU_1                       -   \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "69_tranformerblock_4.leff.layer2.GELU_1                       -   \n",
            "70_tranformerblock_4.leff.layer3.Linear_0            [128, 512]   \n",
            "71_upsample_0.deconv.ConvTranspose2d_0         [256, 512, 2, 2]   \n",
            "72_tranformerblock_5.LayerNorm_norm1                      [512]   \n",
            "73_tranformerblock_5.Dropout_dropout                          -   \n",
            "74_tranformerblock_5.w_msa.Linear_qkv               [512, 1536]   \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop                  -   \n",
            "76_tranformerblock_5.w_msa.Linear_proj               [512, 512]   \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop                  -   \n",
            "78_tranformerblock_5.LayerNorm_norm2                      [512]   \n",
            "79_tranformerblock_5.Dropout_dropout                          -   \n",
            "80_tranformerblock_5.leff.layer1.Linear_0            [512, 128]   \n",
            "81_tranformerblock_5.leff.layer1.GELU_1                       -   \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "83_tranformerblock_5.leff.layer2.GELU_1                       -   \n",
            "84_tranformerblock_5.leff.layer3.Linear_0            [128, 512]   \n",
            "85_upsample_1.deconv.ConvTranspose2d_0         [128, 512, 2, 2]   \n",
            "86_tranformerblock_6.LayerNorm_norm1                      [256]   \n",
            "87_tranformerblock_6.Dropout_dropout                          -   \n",
            "88_tranformerblock_6.w_msa.Linear_qkv                [256, 768]   \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop                  -   \n",
            "90_tranformerblock_6.w_msa.Linear_proj               [256, 256]   \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop                  -   \n",
            "92_tranformerblock_6.LayerNorm_norm2                      [256]   \n",
            "93_tranformerblock_6.Dropout_dropout                          -   \n",
            "94_tranformerblock_6.leff.layer1.Linear_0            [256, 128]   \n",
            "95_tranformerblock_6.leff.layer1.GELU_1                       -   \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "97_tranformerblock_6.leff.layer2.GELU_1                       -   \n",
            "98_tranformerblock_6.leff.layer3.Linear_0            [128, 256]   \n",
            "99_upsample_2.deconv.ConvTranspose2d_0          [64, 256, 2, 2]   \n",
            "100_tranformerblock_7.LayerNorm_norm1                     [128]   \n",
            "101_tranformerblock_7.Dropout_dropout                         -   \n",
            "102_tranformerblock_7.w_msa.Linear_qkv               [128, 384]   \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop                 -   \n",
            "104_tranformerblock_7.w_msa.Linear_proj              [128, 128]   \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop                 -   \n",
            "106_tranformerblock_7.LayerNorm_norm2                     [128]   \n",
            "107_tranformerblock_7.Dropout_dropout                         -   \n",
            "108_tranformerblock_7.leff.layer1.Linear_0           [128, 128]   \n",
            "109_tranformerblock_7.leff.layer1.GELU_1                      -   \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0     [128, 128, 3, 3]   \n",
            "111_tranformerblock_7.leff.layer2.GELU_1                      -   \n",
            "112_tranformerblock_7.leff.layer3.Linear_0           [128, 128]   \n",
            "113_upsample_3.deconv.ConvTranspose2d_0         [32, 128, 2, 2]   \n",
            "114_tranformerblock_8.LayerNorm_norm1                      [64]   \n",
            "115_tranformerblock_8.Dropout_dropout                         -   \n",
            "116_tranformerblock_8.w_msa.Linear_qkv                [64, 192]   \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop                 -   \n",
            "118_tranformerblock_8.w_msa.Linear_proj                [64, 64]   \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop                 -   \n",
            "120_tranformerblock_8.LayerNorm_norm2                      [64]   \n",
            "121_tranformerblock_8.Dropout_dropout                         -   \n",
            "122_tranformerblock_8.leff.layer1.Linear_0            [64, 128]   \n",
            "123_tranformerblock_8.leff.layer1.GELU_1                      -   \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0     [128, 128, 3, 3]   \n",
            "125_tranformerblock_8.leff.layer2.GELU_1                      -   \n",
            "126_tranformerblock_8.leff.layer3.Linear_0            [128, 64]   \n",
            "127_output_proj.proj.Conv2d_0                     [64, 3, 3, 3]   \n",
            "128_output_proj.proj.LeakyReLU_1                              -   \n",
            "\n",
            "                                                  Output Shape     Params  \\\n",
            "Layer                                                                       \n",
            "0_input_proj.proj.Conv2d_0                     [8, 32, 64, 16]      896.0   \n",
            "1_input_proj.proj.LeakyReLU_1                  [8, 32, 64, 16]          -   \n",
            "2_tranformerblock_0.LayerNorm_norm1              [8, 1024, 32]       64.0   \n",
            "3_tranformerblock_0.Dropout_dropout              [8, 1024, 32]          -   \n",
            "4_tranformerblock_0.w_msa.Linear_qkv             [8, 1024, 96]     3.072k   \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop    [8, 1024, 1024]          -   \n",
            "6_tranformerblock_0.w_msa.Linear_proj            [8, 1024, 32]     1.056k   \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop      [8, 1024, 32]          -   \n",
            "8_tranformerblock_0.LayerNorm_norm2              [8, 1024, 32]       64.0   \n",
            "9_tranformerblock_0.Dropout_dropout              [8, 1024, 32]          -   \n",
            "10_tranformerblock_0.leff.layer1.Linear_0       [8, 1024, 128]     4.224k   \n",
            "11_tranformerblock_0.leff.layer1.GELU_1         [8, 1024, 128]          -   \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0       [128, 1024, 8]   147.584k   \n",
            "13_tranformerblock_0.leff.layer2.GELU_1         [128, 1024, 8]          -   \n",
            "14_tranformerblock_0.leff.layer3.Linear_0        [8, 1024, 32]     4.128k   \n",
            "15_downsample_0.conv.Conv2d_0                  [8, 64, 16, 16]    32.832k   \n",
            "16_tranformerblock_1.LayerNorm_norm1              [8, 256, 64]      128.0   \n",
            "17_tranformerblock_1.Dropout_dropout              [8, 256, 64]          -   \n",
            "18_tranformerblock_1.w_msa.Linear_qkv            [8, 256, 192]    12.288k   \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop     [8, 512, 512]          -   \n",
            "20_tranformerblock_1.w_msa.Linear_proj            [8, 256, 64]      4.16k   \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop      [8, 256, 64]          -   \n",
            "22_tranformerblock_1.LayerNorm_norm2              [8, 256, 64]      128.0   \n",
            "23_tranformerblock_1.Dropout_dropout              [8, 256, 64]          -   \n",
            "24_tranformerblock_1.leff.layer1.Linear_0        [8, 256, 128]      8.32k   \n",
            "25_tranformerblock_1.leff.layer1.GELU_1          [8, 256, 128]          -   \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0        [128, 256, 8]   147.584k   \n",
            "27_tranformerblock_1.leff.layer2.GELU_1          [128, 256, 8]          -   \n",
            "28_tranformerblock_1.leff.layer3.Linear_0         [8, 256, 64]     8.256k   \n",
            "29_downsample_1.conv.Conv2d_0                   [8, 128, 8, 8]     131.2k   \n",
            "30_tranformerblock_2.LayerNorm_norm1              [8, 64, 128]      256.0   \n",
            "31_tranformerblock_2.Dropout_dropout              [8, 64, 128]          -   \n",
            "32_tranformerblock_2.w_msa.Linear_qkv             [8, 64, 384]    49.152k   \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop     [8, 256, 256]          -   \n",
            "34_tranformerblock_2.w_msa.Linear_proj            [8, 64, 128]    16.512k   \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop      [8, 64, 128]          -   \n",
            "36_tranformerblock_2.LayerNorm_norm2              [8, 64, 128]      256.0   \n",
            "37_tranformerblock_2.Dropout_dropout              [8, 64, 128]          -   \n",
            "38_tranformerblock_2.leff.layer1.Linear_0         [8, 64, 128]    16.512k   \n",
            "39_tranformerblock_2.leff.layer1.GELU_1           [8, 64, 128]          -   \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0         [128, 64, 8]   147.584k   \n",
            "41_tranformerblock_2.leff.layer2.GELU_1           [128, 64, 8]          -   \n",
            "42_tranformerblock_2.leff.layer3.Linear_0         [8, 64, 128]    16.512k   \n",
            "43_downsample_2.conv.Conv2d_0                   [8, 256, 4, 4]   524.544k   \n",
            "44_tranformerblock_3.LayerNorm_norm1              [8, 16, 256]      512.0   \n",
            "45_tranformerblock_3.Dropout_dropout              [8, 16, 256]          -   \n",
            "46_tranformerblock_3.w_msa.Linear_qkv             [8, 16, 768]   196.608k   \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop     [8, 128, 128]          -   \n",
            "48_tranformerblock_3.w_msa.Linear_proj            [8, 16, 256]    65.792k   \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop      [8, 16, 256]          -   \n",
            "50_tranformerblock_3.LayerNorm_norm2              [8, 16, 256]      512.0   \n",
            "51_tranformerblock_3.Dropout_dropout              [8, 16, 256]          -   \n",
            "52_tranformerblock_3.leff.layer1.Linear_0         [8, 16, 128]    32.896k   \n",
            "53_tranformerblock_3.leff.layer1.GELU_1           [8, 16, 128]          -   \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0         [128, 16, 8]   147.584k   \n",
            "55_tranformerblock_3.leff.layer2.GELU_1           [128, 16, 8]          -   \n",
            "56_tranformerblock_3.leff.layer3.Linear_0         [8, 16, 256]    33.024k   \n",
            "57_downsample_3.conv.Conv2d_0                   [8, 512, 2, 2]  2.097664M   \n",
            "58_tranformerblock_4.LayerNorm_norm1               [8, 4, 512]     1.024k   \n",
            "59_tranformerblock_4.Dropout_dropout               [8, 4, 512]          -   \n",
            "60_tranformerblock_4.w_msa.Linear_qkv             [8, 4, 1536]   786.432k   \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop       [8, 64, 64]          -   \n",
            "62_tranformerblock_4.w_msa.Linear_proj             [8, 4, 512]   262.656k   \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop       [8, 4, 512]          -   \n",
            "64_tranformerblock_4.LayerNorm_norm2               [8, 4, 512]     1.024k   \n",
            "65_tranformerblock_4.Dropout_dropout               [8, 4, 512]          -   \n",
            "66_tranformerblock_4.leff.layer1.Linear_0          [8, 4, 128]    65.664k   \n",
            "67_tranformerblock_4.leff.layer1.GELU_1            [8, 4, 128]          -   \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0          [128, 4, 8]   147.584k   \n",
            "69_tranformerblock_4.leff.layer2.GELU_1            [128, 4, 8]          -   \n",
            "70_tranformerblock_4.leff.layer3.Linear_0          [8, 4, 512]    66.048k   \n",
            "71_upsample_0.deconv.ConvTranspose2d_0          [8, 256, 4, 4]   524.544k   \n",
            "72_tranformerblock_5.LayerNorm_norm1              [8, 16, 512]     1.024k   \n",
            "73_tranformerblock_5.Dropout_dropout              [8, 16, 512]          -   \n",
            "74_tranformerblock_5.w_msa.Linear_qkv            [8, 16, 1536]   786.432k   \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop     [8, 256, 256]          -   \n",
            "76_tranformerblock_5.w_msa.Linear_proj            [8, 16, 512]   262.656k   \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop      [8, 16, 512]          -   \n",
            "78_tranformerblock_5.LayerNorm_norm2              [8, 16, 512]     1.024k   \n",
            "79_tranformerblock_5.Dropout_dropout              [8, 16, 512]          -   \n",
            "80_tranformerblock_5.leff.layer1.Linear_0         [8, 16, 128]    65.664k   \n",
            "81_tranformerblock_5.leff.layer1.GELU_1           [8, 16, 128]          -   \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0         [128, 16, 8]   147.584k   \n",
            "83_tranformerblock_5.leff.layer2.GELU_1           [128, 16, 8]          -   \n",
            "84_tranformerblock_5.leff.layer3.Linear_0         [8, 16, 512]    66.048k   \n",
            "85_upsample_1.deconv.ConvTranspose2d_0          [8, 128, 8, 8]   262.272k   \n",
            "86_tranformerblock_6.LayerNorm_norm1              [8, 64, 256]      512.0   \n",
            "87_tranformerblock_6.Dropout_dropout              [8, 64, 256]          -   \n",
            "88_tranformerblock_6.w_msa.Linear_qkv             [8, 64, 768]   196.608k   \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop     [8, 512, 512]          -   \n",
            "90_tranformerblock_6.w_msa.Linear_proj            [8, 64, 256]    65.792k   \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop      [8, 64, 256]          -   \n",
            "92_tranformerblock_6.LayerNorm_norm2              [8, 64, 256]      512.0   \n",
            "93_tranformerblock_6.Dropout_dropout              [8, 64, 256]          -   \n",
            "94_tranformerblock_6.leff.layer1.Linear_0         [8, 64, 128]    32.896k   \n",
            "95_tranformerblock_6.leff.layer1.GELU_1           [8, 64, 128]          -   \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0         [128, 64, 8]   147.584k   \n",
            "97_tranformerblock_6.leff.layer2.GELU_1           [128, 64, 8]          -   \n",
            "98_tranformerblock_6.leff.layer3.Linear_0         [8, 64, 256]    33.024k   \n",
            "99_upsample_2.deconv.ConvTranspose2d_0         [8, 64, 16, 16]      65.6k   \n",
            "100_tranformerblock_7.LayerNorm_norm1            [8, 256, 128]      256.0   \n",
            "101_tranformerblock_7.Dropout_dropout            [8, 256, 128]          -   \n",
            "102_tranformerblock_7.w_msa.Linear_qkv           [8, 256, 384]    49.152k   \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop  [8, 1024, 1024]          -   \n",
            "104_tranformerblock_7.w_msa.Linear_proj          [8, 256, 128]    16.512k   \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop    [8, 256, 128]          -   \n",
            "106_tranformerblock_7.LayerNorm_norm2            [8, 256, 128]      256.0   \n",
            "107_tranformerblock_7.Dropout_dropout            [8, 256, 128]          -   \n",
            "108_tranformerblock_7.leff.layer1.Linear_0       [8, 256, 128]    16.512k   \n",
            "109_tranformerblock_7.leff.layer1.GELU_1         [8, 256, 128]          -   \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0       [128, 256, 8]   147.584k   \n",
            "111_tranformerblock_7.leff.layer2.GELU_1         [128, 256, 8]          -   \n",
            "112_tranformerblock_7.leff.layer3.Linear_0       [8, 256, 128]    16.512k   \n",
            "113_upsample_3.deconv.ConvTranspose2d_0        [8, 32, 32, 32]    16.416k   \n",
            "114_tranformerblock_8.LayerNorm_norm1            [8, 1024, 64]      128.0   \n",
            "115_tranformerblock_8.Dropout_dropout            [8, 1024, 64]          -   \n",
            "116_tranformerblock_8.w_msa.Linear_qkv          [8, 1024, 192]    12.288k   \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop  [8, 2048, 2048]          -   \n",
            "118_tranformerblock_8.w_msa.Linear_proj          [8, 1024, 64]      4.16k   \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop    [8, 1024, 64]          -   \n",
            "120_tranformerblock_8.LayerNorm_norm2            [8, 1024, 64]      128.0   \n",
            "121_tranformerblock_8.Dropout_dropout            [8, 1024, 64]          -   \n",
            "122_tranformerblock_8.leff.layer1.Linear_0      [8, 1024, 128]      8.32k   \n",
            "123_tranformerblock_8.leff.layer1.GELU_1        [8, 1024, 128]          -   \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0      [128, 1024, 8]   147.584k   \n",
            "125_tranformerblock_8.leff.layer2.GELU_1        [128, 1024, 8]          -   \n",
            "126_tranformerblock_8.leff.layer3.Linear_0       [8, 1024, 64]     8.256k   \n",
            "127_output_proj.proj.Conv2d_0                   [8, 3, 32, 32]     1.731k   \n",
            "128_output_proj.proj.LeakyReLU_1                [8, 3, 32, 32]          -   \n",
            "\n",
            "                                                Mult-Adds  \n",
            "Layer                                                      \n",
            "0_input_proj.proj.Conv2d_0                       884.736k  \n",
            "1_input_proj.proj.LeakyReLU_1                           -  \n",
            "2_tranformerblock_0.LayerNorm_norm1                  32.0  \n",
            "3_tranformerblock_0.Dropout_dropout                     -  \n",
            "4_tranformerblock_0.w_msa.Linear_qkv               3.072k  \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop             -  \n",
            "6_tranformerblock_0.w_msa.Linear_proj              1.024k  \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop             -  \n",
            "8_tranformerblock_0.LayerNorm_norm2                  32.0  \n",
            "9_tranformerblock_0.Dropout_dropout                     -  \n",
            "10_tranformerblock_0.leff.layer1.Linear_0          4.096k  \n",
            "11_tranformerblock_0.leff.layer1.GELU_1                 -  \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0       1.179648M  \n",
            "13_tranformerblock_0.leff.layer2.GELU_1                 -  \n",
            "14_tranformerblock_0.leff.layer3.Linear_0          4.096k  \n",
            "15_downsample_0.conv.Conv2d_0                   8.388608M  \n",
            "16_tranformerblock_1.LayerNorm_norm1                 64.0  \n",
            "17_tranformerblock_1.Dropout_dropout                    -  \n",
            "18_tranformerblock_1.w_msa.Linear_qkv             12.288k  \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop            -  \n",
            "20_tranformerblock_1.w_msa.Linear_proj             4.096k  \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop            -  \n",
            "22_tranformerblock_1.LayerNorm_norm2                 64.0  \n",
            "23_tranformerblock_1.Dropout_dropout                    -  \n",
            "24_tranformerblock_1.leff.layer1.Linear_0          8.192k  \n",
            "25_tranformerblock_1.leff.layer1.GELU_1                 -  \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0       1.179648M  \n",
            "27_tranformerblock_1.leff.layer2.GELU_1                 -  \n",
            "28_tranformerblock_1.leff.layer3.Linear_0          8.192k  \n",
            "29_downsample_1.conv.Conv2d_0                   8.388608M  \n",
            "30_tranformerblock_2.LayerNorm_norm1                128.0  \n",
            "31_tranformerblock_2.Dropout_dropout                    -  \n",
            "32_tranformerblock_2.w_msa.Linear_qkv             49.152k  \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop            -  \n",
            "34_tranformerblock_2.w_msa.Linear_proj            16.384k  \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop            -  \n",
            "36_tranformerblock_2.LayerNorm_norm2                128.0  \n",
            "37_tranformerblock_2.Dropout_dropout                    -  \n",
            "38_tranformerblock_2.leff.layer1.Linear_0         16.384k  \n",
            "39_tranformerblock_2.leff.layer1.GELU_1                 -  \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0       1.179648M  \n",
            "41_tranformerblock_2.leff.layer2.GELU_1                 -  \n",
            "42_tranformerblock_2.leff.layer3.Linear_0         16.384k  \n",
            "43_downsample_2.conv.Conv2d_0                   8.388608M  \n",
            "44_tranformerblock_3.LayerNorm_norm1                256.0  \n",
            "45_tranformerblock_3.Dropout_dropout                    -  \n",
            "46_tranformerblock_3.w_msa.Linear_qkv            196.608k  \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop            -  \n",
            "48_tranformerblock_3.w_msa.Linear_proj            65.536k  \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop            -  \n",
            "50_tranformerblock_3.LayerNorm_norm2                256.0  \n",
            "51_tranformerblock_3.Dropout_dropout                    -  \n",
            "52_tranformerblock_3.leff.layer1.Linear_0         32.768k  \n",
            "53_tranformerblock_3.leff.layer1.GELU_1                 -  \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0       1.179648M  \n",
            "55_tranformerblock_3.leff.layer2.GELU_1                 -  \n",
            "56_tranformerblock_3.leff.layer3.Linear_0         32.768k  \n",
            "57_downsample_3.conv.Conv2d_0                   8.388608M  \n",
            "58_tranformerblock_4.LayerNorm_norm1                512.0  \n",
            "59_tranformerblock_4.Dropout_dropout                    -  \n",
            "60_tranformerblock_4.w_msa.Linear_qkv            786.432k  \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop            -  \n",
            "62_tranformerblock_4.w_msa.Linear_proj           262.144k  \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop            -  \n",
            "64_tranformerblock_4.LayerNorm_norm2                512.0  \n",
            "65_tranformerblock_4.Dropout_dropout                    -  \n",
            "66_tranformerblock_4.leff.layer1.Linear_0         65.536k  \n",
            "67_tranformerblock_4.leff.layer1.GELU_1                 -  \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0       1.179648M  \n",
            "69_tranformerblock_4.leff.layer2.GELU_1                 -  \n",
            "70_tranformerblock_4.leff.layer3.Linear_0         65.536k  \n",
            "71_upsample_0.deconv.ConvTranspose2d_0          8.388608M  \n",
            "72_tranformerblock_5.LayerNorm_norm1                512.0  \n",
            "73_tranformerblock_5.Dropout_dropout                    -  \n",
            "74_tranformerblock_5.w_msa.Linear_qkv            786.432k  \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop            -  \n",
            "76_tranformerblock_5.w_msa.Linear_proj           262.144k  \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop            -  \n",
            "78_tranformerblock_5.LayerNorm_norm2                512.0  \n",
            "79_tranformerblock_5.Dropout_dropout                    -  \n",
            "80_tranformerblock_5.leff.layer1.Linear_0         65.536k  \n",
            "81_tranformerblock_5.leff.layer1.GELU_1                 -  \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0       1.179648M  \n",
            "83_tranformerblock_5.leff.layer2.GELU_1                 -  \n",
            "84_tranformerblock_5.leff.layer3.Linear_0         65.536k  \n",
            "85_upsample_1.deconv.ConvTranspose2d_0         16.777216M  \n",
            "86_tranformerblock_6.LayerNorm_norm1                256.0  \n",
            "87_tranformerblock_6.Dropout_dropout                    -  \n",
            "88_tranformerblock_6.w_msa.Linear_qkv            196.608k  \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop            -  \n",
            "90_tranformerblock_6.w_msa.Linear_proj            65.536k  \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop            -  \n",
            "92_tranformerblock_6.LayerNorm_norm2                256.0  \n",
            "93_tranformerblock_6.Dropout_dropout                    -  \n",
            "94_tranformerblock_6.leff.layer1.Linear_0         32.768k  \n",
            "95_tranformerblock_6.leff.layer1.GELU_1                 -  \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0       1.179648M  \n",
            "97_tranformerblock_6.leff.layer2.GELU_1                 -  \n",
            "98_tranformerblock_6.leff.layer3.Linear_0         32.768k  \n",
            "99_upsample_2.deconv.ConvTranspose2d_0         16.777216M  \n",
            "100_tranformerblock_7.LayerNorm_norm1               128.0  \n",
            "101_tranformerblock_7.Dropout_dropout                   -  \n",
            "102_tranformerblock_7.w_msa.Linear_qkv            49.152k  \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop           -  \n",
            "104_tranformerblock_7.w_msa.Linear_proj           16.384k  \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop           -  \n",
            "106_tranformerblock_7.LayerNorm_norm2               128.0  \n",
            "107_tranformerblock_7.Dropout_dropout                   -  \n",
            "108_tranformerblock_7.leff.layer1.Linear_0        16.384k  \n",
            "109_tranformerblock_7.leff.layer1.GELU_1                -  \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0      1.179648M  \n",
            "111_tranformerblock_7.leff.layer2.GELU_1                -  \n",
            "112_tranformerblock_7.leff.layer3.Linear_0        16.384k  \n",
            "113_upsample_3.deconv.ConvTranspose2d_0        16.777216M  \n",
            "114_tranformerblock_8.LayerNorm_norm1                64.0  \n",
            "115_tranformerblock_8.Dropout_dropout                   -  \n",
            "116_tranformerblock_8.w_msa.Linear_qkv            12.288k  \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop           -  \n",
            "118_tranformerblock_8.w_msa.Linear_proj            4.096k  \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop           -  \n",
            "120_tranformerblock_8.LayerNorm_norm2                64.0  \n",
            "121_tranformerblock_8.Dropout_dropout                   -  \n",
            "122_tranformerblock_8.leff.layer1.Linear_0         8.192k  \n",
            "123_tranformerblock_8.leff.layer1.GELU_1                -  \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0      1.179648M  \n",
            "125_tranformerblock_8.leff.layer2.GELU_1                -  \n",
            "126_tranformerblock_8.leff.layer3.Linear_0         8.192k  \n",
            "127_output_proj.proj.Conv2d_0                   1.769472M  \n",
            "128_output_proj.proj.LeakyReLU_1                        -  \n",
            "-------------------------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params           8.287907M\n",
            "Trainable params       8.287907M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             108.83872M\n",
            "=======================================================================================================\n",
            "The Uformer model has: 8287907 trainable parameters\n",
            "--- Start training: Uformer ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 - Training:   0%|          | 0/14 [00:20<?, ?step/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a7623a8574b9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhardware_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-643e031df5b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/utils/cuda.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loss, optimizer, clip_grad, clip_mode, parameters, create_graph, need_update)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mneed_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     ):\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: derivative for aten::floor_divide is not implemented"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "lV0TTJ-b34SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(device,test_dataloader):\n",
        "  model = Uformer()\n",
        "  model = load_pretrained_model(model,device)\n",
        "  model.to(device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=global_var['RGB_img_res'])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # Evaluate\n",
        "  print(\" --- Begin evaluation --- \")\n",
        "  mean_psnr, mean_ssim = compute_evaluation(test_dataloader,model,device)\n",
        "  print(\" --- End evaluation --- \")\n",
        "  print(\"Mean PSNR: \",mean_psnr)\n",
        "  print(\"Mean SSIM: \",mean_ssim)"
      ],
      "metadata": {
        "id": "DkN_h9E935rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(device,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Q9TdNFPC6x14",
        "outputId": "1ab5a196-41b4-477d-a860-674e49a4f1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-208-4793bb175b32>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'test_dataloader'"
          ]
        }
      ]
    }
  ]
}