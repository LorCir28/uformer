{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_wpJixyu0uG"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtF7IK-nuqlb",
        "outputId": "5e316c2c-9dae-4733-cf07-a662d40ec568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kEN7u0ovvOaU"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Resolutions\n",
        "    'RGB_img_res': (3, 16, 64),\n",
        "\n",
        "    # Parameters\n",
        "    'batch_size': 8,\n",
        "    'n_workers': 2,\n",
        "    'seed': 10000,\n",
        "    'lr': 1e-3,\n",
        "    'lr_patience': 15,\n",
        "    'epochs': 20,\n",
        "    'n_workers': 2,\n",
        "    'e_stop_epochs': 10,\n",
        "\n",
        "    # Operations\n",
        "    'do_print_model': True\n",
        "}\n",
        "\n",
        "augmentation_parameters = {\n",
        "    # TODO\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0hQKBwXMv3yI"
      },
      "outputs": [],
      "source": [
        "dataset_root = '/content/drive/MyDrive/NN_project/SSID_dataset/'\n",
        "save_model_root = '/content/drive/MyDrive/NN_project/'\n",
        "model_name = \"Uformer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KVlRJ0lu7eU"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "tJ2A1RHB2KW_"
      },
      "outputs": [],
      "source": [
        "!pip install einops torchsummaryX --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "x-ZMoEfcu9gM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TT\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchsummaryX import summary\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smh0pVrvtHTZ"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZlZooBR0tO_D"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
        "  valid_mask = y_true > 0.0\n",
        "  valid_pred = y_pred[valid_mask]\n",
        "  valid_true = y_true[valid_mask]\n",
        "  correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
        "  return 100 * torch.mean(correct.float())\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "      return param_group['lr']\n",
        "\n",
        "def hardware_check():\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(\"Actual device: \", device)\n",
        "  return device\n",
        "\n",
        "def load_pretrained_model(model, device):\n",
        "  print(\"Loading checkpoint...\\n\")\n",
        "  model_dict = torch.load(save_model_root+\"/Uformer_best_acc\", map_location=torch.device(device))\n",
        "  model.load_state_dict(model_dict)\n",
        "  print(\"Checkpoint loaded!\\n\")\n",
        "  return model\n",
        "\n",
        "def plot_graph(f, g, f_label, g_label, title, path):\n",
        "  epochs = range(0, len(f))\n",
        "  plt.plot(epochs, f, 'b', label=f_label)\n",
        "  plt.plot(epochs, g, 'orange', label=g_label)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(path + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def plot_history(history):\n",
        "  plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', save_model_root)\n",
        "  plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', save_model_root)\n",
        "\n",
        "def plot_loss(history,title):\n",
        "  l_train_list = history['train_loss']\n",
        "  l_test_list = history['val_loss']\n",
        "  epochs = range(0, len(l_train_list))\n",
        "\n",
        "  plt.plot(epochs, l_train_list, 'r', label='Train loss')\n",
        "  plt.plot(epochs, l_test_list, 'g', label='Test loss')\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(save_model_root + \"/\" + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def print_model(model, device, input_shape):\n",
        "  info = summary(model, torch.ones((global_var['batch_size'], input_shape[0], input_shape[1], input_shape[2])).to(device))\n",
        "  info.to_csv(save_model_root + 'model_summary.csv')\n",
        "\n",
        "def save_checkpoint(model, name):\n",
        "  torch.save(model.state_dict(), save_model_root + name)\n",
        "\n",
        "def save_csv_history(model_name):\n",
        "  objects = []\n",
        "  with (open(save_model_root + model_name + '_history.pkl', \"rb\")) as openfile:\n",
        "      while True:\n",
        "          try:\n",
        "              objects.append(pickle.load(openfile))\n",
        "          except EOFError:\n",
        "              break\n",
        "  df = pd.DataFrame(objects)\n",
        "  df.to_csv(save_model_root + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
        "\n",
        "def save_history(history, filepath):\n",
        "  tmp_file = open(filepath + '.pkl', \"wb\")\n",
        "  pickle.dump(history, tmp_file)\n",
        "  tmp_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXRFscaCxaY7"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oShoEPlgyVxF"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qMmArppAyYRS"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjFnJ-yyaCJ"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LroEZr8QycjJ"
      },
      "outputs": [],
      "source": [
        "class SSID_Dataset(Dataset):\n",
        "    def __init__(self, data_root):\n",
        "        self.dataset_path = data_root\n",
        "        self.dir_list = data_root + \"Scene_Instances.txt\"\n",
        "        self.data_dir = data_root + \"Data/\"\n",
        "        self.data_directiories = []\n",
        "        self.img_paths = []\n",
        "        self.target_paths = []\n",
        "        self.post_processing = TT.Compose([\n",
        "            TT.ToTensor(),\n",
        "            TT.Resize((global_var['RGB_img_res'][1], global_var['RGB_img_res'][2]),antialias=None),\n",
        "        ])\n",
        "\n",
        "        data_dir_file = open(dataset_root+\"Scene_Instances.txt\", 'r')\n",
        "        self.data_directories = [elem.strip() for elem in data_dir_file.readlines()]\n",
        "        data_dir_file.close()\n",
        "\n",
        "        for elem in self.data_directories:\n",
        "          data_path = self.data_dir + elem\n",
        "          content = sorted(os.listdir(data_path))\n",
        "          self.target_paths.append(content[0])\n",
        "          self.img_paths.append(content[1])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.data_dir + self.data_directories[index] + \"/\" + self.img_paths[index]\n",
        "        img = self.post_processing(Image.open(img_path))\n",
        "\n",
        "        target_path = self.data_dir + self.data_directories[index] + \"/\" + self.target_paths[index]\n",
        "        target = self.post_processing(Image.open(target_path))\n",
        "\n",
        "        return img.float(), target.float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyizoOj4yehn"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxVEwFxtyf92",
        "outputId": "7d48cf6c-f1a6-4e49-b8cb-2f75a29a715a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data percentage:  0.7\n",
            "Test data percentage:  0.3\n"
          ]
        }
      ],
      "source": [
        "dataset = SSID_Dataset(dataset_root)\n",
        "train_dataset, test_dataset = random_split(dataset, [112, 48])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size = global_var['batch_size'],\n",
        "                          num_workers = global_var['n_workers'],\n",
        "                          shuffle = True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = global_var['batch_size'],\n",
        "                         num_workers = global_var['n_workers'],\n",
        "                         shuffle = True)\n",
        "\n",
        "print(\"Train data percentage: \", len(train_dataset)/(len(train_dataset)+len(test_dataset)))\n",
        "print(\"Test data percentage: \", len(test_dataset)/(len(train_dataset)+len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr4UgpsTQNXr"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "UGzJ-r8QQO1V"
      },
      "outputs": [],
      "source": [
        "class Cha_loss(nn.Module):\n",
        "  def __init__(self, epsilon=1e-3):\n",
        "    super(Cha_loss,self).__init__()\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def forward(self,pred,truth):\n",
        "    return torch.mean(torch.sqrt((pred-truth)**2 + self.epsilon**2))\n",
        "\n",
        "# class Cha_loss(nn.Module):\n",
        "#     \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "#     def __init__(self, eps=1e-6):\n",
        "#         super(Cha_loss, self).__init__()\n",
        "#         self.eps = eps\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         b, c, h, w = y.size()\n",
        "#         loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
        "#         return loss/(c*b*h*w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maczFOi8eB52"
      },
      "source": [
        "# Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Wm4POhkmeDsJ"
      },
      "outputs": [],
      "source": [
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# It works, compared with torchmetrics.image import PeakSignalNoiseRatio\n",
        "def psnr(original_img, compressed_img, max_pix_val=1.0):\n",
        "  mse = torch.mean((original_img-compressed_img)**2)\n",
        "  return 20 * torch.log10(max_pix_val/torch.sqrt(mse))\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# ATTENTION: 4D tensors needed\n",
        "# It works, compared with StructuralSimilarityIndexMeasure from torchmetrics.image\n",
        "def ssim(original_img, restored_img, max_pix_val=1.0, window_size=11, window=None, size_average=True, full=False):\n",
        "    (_, channel, height, width) = original_img.size()\n",
        "    real_size = min(window_size, height, width)\n",
        "    window = create_window(real_size, channel=channel).to(original_img.device)\n",
        "\n",
        "    mu1 = F.conv2d(original_img, window, padding=0, groups=channel)\n",
        "    mu2 = F.conv2d(restored_img, window, padding=0, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(original_img ** 2, window, padding=0, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(restored_img ** 2, window, padding=0, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(original_img * restored_img, window, padding=0, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * max_pix_val) ** 2\n",
        "    C2 = (0.03 * max_pix_val) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "\n",
        "    return (((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)).mean()\n",
        "\n",
        "\n",
        "def compute_evaluation(test_dataloader, model, device='cpu'):\n",
        "  model.eval()\n",
        "  psnr_values = []\n",
        "  ssim_values = []\n",
        "\n",
        "  for _, (inputs, targets) in enumerate(test_dataloader):\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          predictions = model(inputs)\n",
        "\n",
        "      psnr_values.append(psnr(targets,predictions))\n",
        "      ssim_values.append(ssim(targets,predictions))\n",
        "\n",
        "  return torch.mean(torch.Tensor(psnr_values)).item(),torch.mean(torch.Tensor(ssim_values)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rzLgWkBY50K"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Btr_ivEiY73R"
      },
      "outputs": [],
      "source": [
        "# Attention components\n",
        "# Attention module\n",
        "class W_MSA(nn.Module):\n",
        "  def __init__(self, dim=32, num_heads=8, qkv_bias=False):\n",
        "    super(W_MSA, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = dim // num_heads\n",
        "\n",
        "    # nn.Linear(in_features, out_features): the input of the layer has to have the last dimension equal to in_features (namely (*, in_features)). The output of the layer has the\n",
        "    # same dimension of the input except for the last one which is equal to out_features (namely (*, out_features))\n",
        "\n",
        "    # self.qkv = nn.Linear(dim, num_heads, self.head_dim, bias=qkv_bias) # this layer returns the queries, keys and values\n",
        "    self.qkv = nn.Linear(dim, self.head_dim*self.num_heads*3, bias=qkv_bias)\n",
        "\n",
        "    # these are default layers for the attention module\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(0.)\n",
        "    self.attn_drop = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    # print(\"********* x shape: \",x.shape)\n",
        "    # print(\"********* self qkv shape:\",self.qkv(x).shape)\n",
        "\n",
        "    qkv_temp = self.qkv(x)\n",
        "    mult = qkv_temp.shape[0] * qkv_temp.shape[1] * qkv_temp.shape[2]\n",
        "    mult = mult//3\n",
        "    mult = mult//(self.num_heads*4)\n",
        "    mult = mult//global_var['batch_size']\n",
        "\n",
        "    qkv = qkv_temp.reshape(B, mult, self.num_heads*4, 3)\n",
        "    # print(\"********* qkv shape:\",qkv.shape)\n",
        "    q, k, v = qkv.unbind(dim=-1) # this returnes a tuple of tensors whose each element is portion of the original tensor (qkv) (ref: https://pytorch.org/docs/stable/generated/torch.unbind.html)\n",
        "\n",
        "    # this is the implementation of the attention formula described on the paper\n",
        "    scale = (C // self.head_dim) ** (0.5)\n",
        "    attn = ((q @ k.transpose(-2, -1)) // scale) + B # from the github: the final B can be also removed\n",
        "    attn = attn.softmax(dim=1)\n",
        "    attn = self.attn_drop(attn)\n",
        "    # print(\"********** x no reshape:\",(attn @ v).transpose(1, 2).shape)\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "    # these are default for the attention module\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# this is the simple implementation described in the paper\n",
        "class LeFF(nn.Module):\n",
        "  def __init__(self, dim=32, hidden_dim=128):\n",
        "    super(LeFF, self).__init__()\n",
        "    self.dim = dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.layer1 = nn.Sequential(nn.Linear(dim, hidden_dim), nn.GELU())\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1), nn.GELU())\n",
        "    self.layer3 = nn.Sequential(nn.Linear(hidden_dim, dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(\"Before 1st layer x: \", x.shape)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x.permute(2,1,0))\n",
        "    x = self.layer3(x.permute(2,1,0))\n",
        "\n",
        "    return x\n",
        "\n",
        "  # def forward(self, x):\n",
        "  #   # bs x hw x c\n",
        "  #   bs, hw, c = x.size()\n",
        "  #   hh = int(math.sqrt(hw))\n",
        "\n",
        "  #   x = self.layer1(x)\n",
        "\n",
        "  #   # spatial restore\n",
        "  #   x = rearrange(x, ' b (h w) (c) -> b c h w ', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer2(x)\n",
        "\n",
        "  #   # flatten\n",
        "  #   x = rearrange(x, ' b c h w -> b (h w) c', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer3(x)\n",
        "\n",
        "  #   return x\n",
        "\n",
        "# NN BLOCKS\n",
        "# LeWin Transformer Block (from the paper, it is made up of a sequence of: NormLayer, W_MSA, NormLayer, LeFF)\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.w_msa = W_MSA(dim=dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.leff = LeFF(dim=dim)\n",
        "    self.dropout = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(self.norm1(x))\n",
        "    x = self.w_msa(x)\n",
        "    x = self.dropout(self.norm2(x))\n",
        "    x = self.leff(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Down-sampling Block (reduces the size of the feature map)\n",
        "# reshape the flattened features into 2D spatial feature maps, and then down-sample the maps, double the channels using 4 × 4 convolution with stride 2\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DownsampleBlock, self).__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # remember that x is a tensor!!\n",
        "        B, L, C = x.shape\n",
        "        W = int(math.sqrt(L))*2\n",
        "        # W = int(math.sqrt(L))\n",
        "        H = (((W//4)**2)//(W//2))*2\n",
        "        # print(\"*************** H: \", H)\n",
        "        # print(\"*************** W: \", W)\n",
        "        # print(\"*************** x: \",x.shape)\n",
        "        # print(\"*************** x: \", x.transpose(1, 2).contiguous().shape)\n",
        "        x = x.transpose(1, 2).contiguous().view(B, C, H, W) # this transposes the 1st and 2nd dimension of x, then the size of x is reshaped with view (the new size is (B, C, H, W))\n",
        "                                                            # (.contiguous() is required to make view workable, since view works only on contiguous data)\n",
        "\n",
        "        out = self.conv(x).flatten(2).transpose(1, 2).contiguous() # this pass the input x to the downsample layer, then the 2nd dimension of the output is flattened with the 3rd\n",
        "                                                                   # and finally its 1st and 2nd dimensions are transposed\n",
        "\n",
        "                                                                   # (B, C, H*W) is the size of the out after flatten(2)\n",
        "                                                                   # (B H*W C) is the final size of the out after transpose(1, 2)\n",
        "        # print(\"************** out: \", out.shape)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Up-sampling Block (reduces half of the channels and doubles the size of the feature map)\n",
        "# 2 × 2 transposed convolution with stride 2\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding):\n",
        "      super(UpsampleBlock, self).__init__()\n",
        "      self.in_channel = in_channel\n",
        "      self.out_channel = out_channel\n",
        "      self.deconv = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,padding=padding),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      B, L, C = x.shape\n",
        "      # H = int(math.sqrt(L))\n",
        "      # W = int(math.sqrt(L))\n",
        "      W = int(math.sqrt(L))*2\n",
        "      H = (((W//4)**2)//(W//2))*2\n",
        "      x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "      out = self.deconv(x).flatten(2).transpose(1, 2).contiguous() # B H*W C\n",
        "\n",
        "      return out\n",
        "\n",
        "# Input Projection Block (extracts the low-level features)\n",
        "# 3 x 3 convolutional layer with LeakyReLu\n",
        "class InputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=3, out_channel=32, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C\n",
        "\n",
        "        return x\n",
        "\n",
        "# Output Projection Block (returns the residual)\n",
        "# 3 x 3 convolutional layer\n",
        "class OutputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=64, out_channel=3, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x # the output of this block si called residual\n",
        "\n",
        "# complete uformer class\n",
        "class Uformer(nn.Module):\n",
        "  def __init__(self, embed_dim=32):\n",
        "    super(Uformer, self).__init__()\n",
        "\n",
        "    # encoder\n",
        "    self.input_proj = InputProjBlock()\n",
        "\n",
        "    self.transformerblock_0 = TransformerBlock(embed_dim)\n",
        "    self.downsample_0 = DownsampleBlock(embed_dim, embed_dim*2)\n",
        "\n",
        "    self.transformerblock_1 = TransformerBlock(embed_dim*2)\n",
        "    self.downsample_1 = DownsampleBlock(embed_dim*2, embed_dim*4)\n",
        "\n",
        "    self.transformerblock_2 = TransformerBlock(embed_dim*4)\n",
        "    self.downsample_2 = DownsampleBlock(embed_dim*4, embed_dim*8)\n",
        "\n",
        "    self.transformerblock_3 = TransformerBlock(embed_dim*8)\n",
        "    self.downsample_3 = DownsampleBlock(embed_dim*8, embed_dim*16)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    self.transformerblock_4 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    self.upsample_0 = UpsampleBlock(embed_dim*16, embed_dim*8,kernel_size=2,stride=2,padding=0)\n",
        "    self.transformerblock_5 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "    self.upsample_1 = UpsampleBlock(embed_dim*16, embed_dim*4,kernel_size=2,stride=2,padding=0)\n",
        "    self.transformerblock_6 = TransformerBlock(embed_dim*8)\n",
        "\n",
        "    self.upsample_2 = UpsampleBlock(embed_dim*8, embed_dim*2,kernel_size=2,stride=2,padding=0)\n",
        "    self.transformerblock_7 = TransformerBlock(embed_dim*4)\n",
        "\n",
        "    self.upsample_3 = UpsampleBlock(embed_dim*4, embed_dim,kernel_size=2,stride=2,padding=0)\n",
        "    self.transformerblock_8 = TransformerBlock(embed_dim*2)\n",
        "\n",
        "    self.output_proj = OutputProjBlock()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    degraded_image = x # x is the degraded image\n",
        "\n",
        "\n",
        "    # encoder\n",
        "    y = self.input_proj(x)\n",
        "    t0 = self.transformerblock_0(y)\n",
        "    d0 = self.downsample_0(t0)\n",
        "    t1 = self.transformerblock_1(d0)\n",
        "    d1 = self.downsample_1(t1)\n",
        "    t2 = self.transformerblock_2(d1)\n",
        "    d2 = self.downsample_2(t2)\n",
        "    t3 = self.transformerblock_3(d2)\n",
        "    d3 = self.downsample_3(t3)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    t4 = self.transformerblock_4(d3)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    u0 = self.upsample_0(t4)\n",
        "    # print(\"1) Upsampled in: \",u0.shape)\n",
        "    # print(\"1) Skipped in: \",t3.shape)\n",
        "    skippedconn_0 = torch.cat([u0, t3], -1) # this creates a skipped connection between t3 and t6 (u0 would have to be the input of t5)\n",
        "    t5 = self.transformerblock_5(skippedconn_0)\n",
        "\n",
        "    u1 = self.upsample_1(t5)\n",
        "    # print(\"2) Upsampled in: \",u1.shape)\n",
        "    # print(\"2) Skipped in: \",t2.shape)\n",
        "    skippedconn_1 = torch.cat([u1, t2], -1)\n",
        "    t6 = self.transformerblock_6(skippedconn_1)\n",
        "\n",
        "    u2 = self.upsample_2(t6)\n",
        "    # print(\"3) Upsampled in: \",u2.shape)\n",
        "    # print(\"3) Skipped in: \",t1.shape)\n",
        "    skippedconn_2 = torch.cat([u2, t1], -1)\n",
        "    t7 = self.transformerblock_7(skippedconn_2)\n",
        "\n",
        "    u3 = self.upsample_3(t7)\n",
        "    # print(\"4) Upsampled in: \",u3.shape)\n",
        "    # print(\"4) Skipped in: \",t0.shape)\n",
        "    skippedconn_3 = torch.cat([u3, t0], -1)\n",
        "    t8 = self.transformerblock_8(skippedconn_3)\n",
        "\n",
        "    residual = self.output_proj(t8)\n",
        "\n",
        "\n",
        "    # final residual summation\n",
        "    # print(\"Degraded: \",degraded_image.shape)\n",
        "    # print(\"Residual: \",residual.shape)\n",
        "    restored_image = degraded_image + residual.reshape([residual.shape[0],residual.shape[1],degraded_image.shape[2],degraded_image.shape[3]])\n",
        "\n",
        "    return restored_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCfDRBKpV_y"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7IkSzCVHpZxp"
      },
      "outputs": [],
      "source": [
        "def train(device,train_dataloader,test_dataloader):\n",
        "  # Set-seed\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "  np.random.seed(global_var['seed'])\n",
        "  torch.cuda.manual_seed(global_var['seed'])\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Globals\n",
        "  history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': []}\n",
        "  min_acc = 0\n",
        "  train_acc_list = []\n",
        "  test_acc_list = []\n",
        "  train_loss_list = []\n",
        "  test_loss_list = []\n",
        "\n",
        "  # Loss\n",
        "  criterion = Cha_loss()\n",
        "\n",
        "  # Model\n",
        "  model = Uformer()\n",
        "  model.to(device=device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=global_var['RGB_img_res'])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # # Optimizer\n",
        "  # optimizer = torch.optim.AdamW(\n",
        "  #   model.parameters(), lr=global_var['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
        "  # )\n",
        "\n",
        "  # # Scheduler\n",
        "  # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "  #   optimizer, mode='min', factor=0.1, patience=global_var['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
        "  #   cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
        "  # )\n",
        "\n",
        "  # # Early stopping\n",
        "  # trigger_times, early_stopping_epochs = 0, global_var['e_stop_epochs']\n",
        "\n",
        "  # print(\"--- Start training: {} ---\\n\".format(model_name))\n",
        "  # # Train\n",
        "\n",
        "  # for epoch in range(global_var['epochs']):\n",
        "  #   iter = 1\n",
        "  #   model.train(mode=True)\n",
        "  #   running_loss, accuracy = 0, 0\n",
        "\n",
        "  #   with tqdm(train_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "  #     for batch in tepoch:\n",
        "  #       tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Training\")\n",
        "\n",
        "  #       # Load data\n",
        "  #       inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "  #       # Forward\n",
        "  #       optimizer.zero_grad()\n",
        "  #       outputs = model(inputs)\n",
        "\n",
        "  #       # Compute loss\n",
        "  #       loss = criterion(outputs, targets)\n",
        "\n",
        "  #       # Backward\n",
        "  #       loss = torch.clone(loss).detach().requires_grad_(True)\n",
        "  #       loss.backward()\n",
        "  #       optimizer.step()\n",
        "\n",
        "  #       # Evaluation and Stats\n",
        "  #       running_loss += loss.item()\n",
        "  #       train_loss_list.append(loss.item())\n",
        "\n",
        "  #       accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "  #       tepoch.set_postfix({'Loss': running_loss / iter,\n",
        "  #                           'Acc': accuracy.item() / iter,\n",
        "  #                           'Lr': global_var['lr'] if not history['lrs'] else history['lrs'][-1]})\n",
        "  #       iter += 1\n",
        "\n",
        "  #   # Validation\n",
        "  #   iter = 1\n",
        "  #   model.eval()\n",
        "  #   test_loss, test_accuracy = 0, 0\n",
        "  #   with tqdm(test_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "  #     for batch in tepoch:\n",
        "  #       tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Validation\")\n",
        "  #       inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "  #       # Validation loop\n",
        "  #       with torch.no_grad():\n",
        "  #         outputs = model(inputs)\n",
        "\n",
        "  #         # Evaluation metrics\n",
        "  #         test_accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "  #         # Loss\n",
        "  #         loss = criterion(outputs, targets)\n",
        "  #         test_loss += loss.item()\n",
        "  #         test_loss_list.append(loss.item())\n",
        "\n",
        "  #         tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter})\n",
        "  #         iter += 1\n",
        "\n",
        "  #       # Update history infos\n",
        "  #       history['lrs'].append(get_lr(optimizer))\n",
        "  #       history['train_loss'].append(running_loss / len(train_dataloader))\n",
        "  #       history['val_loss'].append(test_loss / len(test_dataloader))\n",
        "  #       history['train_acc'].append(accuracy.item() / len(train_dataloader))\n",
        "  #       history['val_acc'].append(test_accuracy.item() / len(test_dataloader))\n",
        "\n",
        "  #       # Save model by best ACCURACY\n",
        "  #       if min_acc <= (test_accuracy / len(test_dataloader)):\n",
        "  #         min_acc = test_accuracy / len(test_dataloader)\n",
        "  #         save_checkpoint(model, model_name + '_best_acc')\n",
        "  #         print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
        "\n",
        "  #         if trigger_times > 4:\n",
        "  #           trigger_times = trigger_times - 2\n",
        "  #           print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
        "\n",
        "\n",
        "  #       save_history(history, save_model_root + model_name + '_history')\n",
        "  #       # Empty CUDA cache\n",
        "  #       torch.cuda.empty_cache()\n",
        "\n",
        "  #       if trigger_times == early_stopping_epochs:\n",
        "  #           print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
        "  #           break\n",
        "\n",
        "  #       # Save loss for graphs\n",
        "  #       np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
        "  #       np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
        "\n",
        "  # print('--- Finished Training ---')\n",
        "  # save_csv_history(model_name=model_name)\n",
        "  # plot_history(history)\n",
        "  # plot_loss(history, title='Loss Trend')\n",
        "\n",
        "  # return history, min_acc, train_acc_list, test_acc_list, train_loss_list, test_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "ZQvL4hihzUer",
        "outputId": "07bf9dba-cd7a-4e6c-eb39-0426aa1565fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual device:  cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-037ae5f01707>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhardware_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-0bd6c110372d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do_print_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprint_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RGB_img_res'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The {} model has: {} trainable parameters'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-11e249d8d471>\u001b[0m in \u001b[0;36mprint_model\u001b[0;34m(model, device, input_shape)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'model_summary.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-6acd28f9534b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;31m# print(\"1) Upsampled in: \",u0.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# print(\"1) Skipped in: \",t3.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-6acd28f9534b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B H*W C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 512, 0, 4]' is invalid for input of size 16384"
          ]
        }
      ],
      "source": [
        "device = hardware_check()\n",
        "stats = train(device,train_loader,test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV0TTJ-b34SV"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkN_h9E935rb"
      },
      "outputs": [],
      "source": [
        "def test(device,test_dataloader):\n",
        "  model = Uformer()\n",
        "  model = load_pretrained_model(model,device)\n",
        "  model.to(device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=global_var['RGB_img_res'])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # Evaluate\n",
        "  print(\" --- Begin evaluation --- \")\n",
        "  mean_psnr, mean_ssim = compute_evaluation(test_dataloader,model,device)\n",
        "  print(\" --- End evaluation --- \")\n",
        "  print(\"Mean PSNR: \",mean_psnr)\n",
        "  print(\"Mean SSIM: \",mean_ssim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9TdNFPC6x14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31b9884-9407-46ee-f608-bfd94ec031d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint...\n",
            "\n",
            "Checkpoint loaded!\n",
            "\n",
            " --- Begin evaluation --- \n",
            " --- End evaluation --- \n",
            "Mean PSNR:  24.364336013793945\n",
            "Mean SSIM:  0.8439557552337646\n"
          ]
        }
      ],
      "source": [
        "test(device,test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5KVlRJ0lu7eU",
        "smh0pVrvtHTZ",
        "wr4UgpsTQNXr",
        "maczFOi8eB52",
        "lV0TTJ-b34SV"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}