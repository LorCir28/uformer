{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_wpJixyu0uG"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtF7IK-nuqlb",
        "outputId": "b3f0fc32-8baa-40d4-bce6-f3da0e04ce81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "kEN7u0ovvOaU"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Resolutions\n",
        "    'RGB_img_res': (3, 16, 64), # Tensor.shape = [Batch, Channel, Height, Width]\n",
        "    'patch_size' : 32,\n",
        "\n",
        "    # Parameters\n",
        "    'batch_size': 32,\n",
        "    'eval_batch_size': 1,\n",
        "    'n_workers': 2,\n",
        "    'seed': 10000,\n",
        "    'lr': 2e-4,\n",
        "    'lr_patience': 15,\n",
        "    'epochs': 50,\n",
        "    'n_workers': 2,\n",
        "    'e_stop_epochs': 10,\n",
        "\n",
        "    # Operations\n",
        "    'do_print_model': True\n",
        "}\n",
        "\n",
        "augmentation_parameters = {\n",
        "    # TODO\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "0hQKBwXMv3yI"
      },
      "outputs": [],
      "source": [
        "dataset_root = '/content/drive/MyDrive/NN_project/SSID_dataset/'\n",
        "save_model_root = '/content/drive/MyDrive/NN_project/'\n",
        "model_name = \"Uformer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KVlRJ0lu7eU"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "tJ2A1RHB2KW_"
      },
      "outputs": [],
      "source": [
        "!pip install einops torchsummaryX --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "x-ZMoEfcu9gM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TT\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchsummaryX import summary\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smh0pVrvtHTZ"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "ZlZooBR0tO_D"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
        "  valid_mask = y_true > 0.0\n",
        "  valid_pred = y_pred[valid_mask]\n",
        "  valid_true = y_true[valid_mask]\n",
        "  correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
        "  return 100 * torch.mean(correct.float())\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "      return param_group['lr']\n",
        "\n",
        "def hardware_check():\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(\"Actual device: \", device)\n",
        "  return device\n",
        "\n",
        "def load_pretrained_model(model, device):\n",
        "  print(\"Loading checkpoint...\\n\")\n",
        "  model_dict = torch.load(save_model_root+\"/Uformer_best_acc\", map_location=torch.device(device))\n",
        "  model.load_state_dict(model_dict)\n",
        "  print(\"Checkpoint loaded!\\n\")\n",
        "  return model\n",
        "\n",
        "def plot_graph(f, g, f_label, g_label, title, path):\n",
        "  epochs = range(0, len(f))\n",
        "  plt.plot(epochs, f, 'b', label=f_label)\n",
        "  plt.plot(epochs, g, 'orange', label=g_label)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(path + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def plot_history(history):\n",
        "  plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', save_model_root)\n",
        "  plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', save_model_root)\n",
        "\n",
        "def plot_loss(history,title):\n",
        "  l_train_list = history['train_loss']\n",
        "  l_test_list = history['val_loss']\n",
        "  epochs = range(0, len(global_var['epochs']))\n",
        "\n",
        "  plt.plot(epochs, l_train_list, 'r', label='Train loss')\n",
        "  plt.plot(epochs, l_test_list, 'g', label='Test loss')\n",
        "  plt.title(title)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.grid('on', color='#cfcfcf')\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(save_model_root + \"/\" + title + '.pdf')\n",
        "  plt.close()\n",
        "\n",
        "def print_model(model, device, input_shape):\n",
        "  info = summary(model, torch.ones((global_var['batch_size'], input_shape[0], input_shape[1], input_shape[2])).to(device))\n",
        "  info.to_csv(save_model_root + 'model_summary.csv')\n",
        "\n",
        "def save_checkpoint(model, name):\n",
        "  torch.save(model.state_dict(), save_model_root + name)\n",
        "\n",
        "def save_csv_history(model_name):\n",
        "  objects = []\n",
        "  with (open(save_model_root + model_name + '_history.pkl', \"rb\")) as openfile:\n",
        "      while True:\n",
        "          try:\n",
        "              objects.append(pickle.load(openfile))\n",
        "          except EOFError:\n",
        "              break\n",
        "  df = pd.DataFrame(objects)\n",
        "  df.to_csv(save_model_root + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
        "\n",
        "def save_history(history, filepath):\n",
        "  tmp_file = open(filepath + '.pkl', \"wb\")\n",
        "  pickle.dump(history, tmp_file)\n",
        "  tmp_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXRFscaCxaY7"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oShoEPlgyVxF"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "qMmArppAyYRS"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjFnJ-yyaCJ"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "LroEZr8QycjJ"
      },
      "outputs": [],
      "source": [
        "from tables.index import idx2long\n",
        "class SSID_Dataset(Dataset):\n",
        "  def __init__(self, data_root):\n",
        "    assert os.path.exists(data_root)\n",
        "    super(SSID_Dataset, self).__init__()\n",
        "\n",
        "    self.instances_list_path = data_root + \"Scene_Instances.txt\"\n",
        "    self.data_directories_path = data_root + \"Data/\"\n",
        "\n",
        "    self.img_paths = []\n",
        "    self.target_paths = []\n",
        "    self.dataset_size = 0\n",
        "\n",
        "    instances_list_file = open(self.instances_list_path, 'r')\n",
        "    data_directories = [self.data_directories_path + elem.strip()+\"/\" for elem in instances_list_file.readlines()]\n",
        "    instances_list_file.close()\n",
        "\n",
        "    for elem in data_directories:\n",
        "      content = sorted(os.listdir(elem))\n",
        "      self.target_paths.append(elem+content[0])\n",
        "      self.img_paths.append(elem+content[1])\n",
        "\n",
        "    self.dataset_size = len(self.img_paths)\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.dataset_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      idx = index % self.dataset_size # Avoid going beyond limits\n",
        "\n",
        "      raw_noise_img = np.float32(cv2.cvtColor(cv2.imread(self.img_paths[idx]), cv2.COLOR_BGR2RGB))\n",
        "      noise_img = torch.from_numpy(raw_noise_img).permute(2,0,1)\n",
        "\n",
        "      raw_gt_img = np.float32(cv2.cvtColor(cv2.imread(self.target_paths[idx]), cv2.COLOR_BGR2RGB))\n",
        "      gt_img = torch.from_numpy(raw_gt_img).permute(2,0,1)\n",
        "\n",
        "      ps = global_var['patch_size']\n",
        "      H = gt_img.shape[1]\n",
        "      W = gt_img.shape[2]\n",
        "      # r = np.random.randint(0, H - ps) if not H-ps else 0\n",
        "      # c = np.random.randint(0, W - ps) if not H-ps else 0\n",
        "      if H-ps==0:\n",
        "          r=0\n",
        "          c=0\n",
        "      else:\n",
        "          r = np.random.randint(0, H - ps)\n",
        "          c = np.random.randint(0, W - ps)\n",
        "      gt_img = gt_img[:, r:r + ps, c:c + ps]\n",
        "      noise_img = noise_img[:, r:r + ps, c:c + ps]\n",
        "\n",
        "      return noise_img, gt_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyizoOj4yehn"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxVEwFxtyf92",
        "outputId": "797fd318-5847-4f71-89c7-2e4ad0472888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data percentage:  0.8\n",
            "Test data percentage:  0.2\n"
          ]
        }
      ],
      "source": [
        "dataset = SSID_Dataset(dataset_root)\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [128, 32])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size = global_var['batch_size'],\n",
        "                          num_workers = global_var['n_workers'],\n",
        "                          shuffle = True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size = global_var['eval_batch_size'],\n",
        "                         num_workers = global_var['n_workers'],\n",
        "                         shuffle = True)\n",
        "\n",
        "print(\"Train data percentage: \", len(train_dataset)/(len(train_dataset)+len(test_dataset)))\n",
        "print(\"Test data percentage: \", len(test_dataset)/(len(train_dataset)+len(test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr4UgpsTQNXr"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "UGzJ-r8QQO1V"
      },
      "outputs": [],
      "source": [
        "class Cha_loss(nn.Module):\n",
        "  def __init__(self, epsilon=1e-3):\n",
        "    super(Cha_loss,self).__init__()\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def forward(self,pred,truth):\n",
        "    return torch.mean(torch.sqrt((pred-truth)**2 + self.epsilon**2))\n",
        "\n",
        "# class Cha_loss(nn.Module):\n",
        "#     \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "#     def __init__(self, eps=1e-6):\n",
        "#         super(Cha_loss, self).__init__()\n",
        "#         self.eps = eps\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         b, c, h, w = y.size()\n",
        "#         loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
        "#         return loss/(c*b*h*w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maczFOi8eB52"
      },
      "source": [
        "# Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "Wm4POhkmeDsJ"
      },
      "outputs": [],
      "source": [
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# It works, compared with torchmetrics.image import PeakSignalNoiseRatio\n",
        "def psnr(original_img, compressed_img, max_pix_val=1.0):\n",
        "  mse = torch.mean((original_img-compressed_img)**2)\n",
        "  return 20 * torch.log10(max_pix_val/torch.sqrt(mse))\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "# ATTENTION: PYTORCH HAS PIXEL RANGE BETWEEN 0.0 AND 1.0, NOT BETWEEN 0 AND 255\n",
        "# ATTENTION: 4D tensors needed\n",
        "# It works, compared with StructuralSimilarityIndexMeasure from torchmetrics.image\n",
        "def ssim(original_img, restored_img, max_pix_val=1.0, window_size=11, window=None, size_average=True, full=False):\n",
        "    (_, channel, height, width) = original_img.size()\n",
        "    real_size = min(window_size, height, width)\n",
        "    window = create_window(real_size, channel=channel).to(original_img.device)\n",
        "\n",
        "    mu1 = F.conv2d(original_img, window, padding=0, groups=channel)\n",
        "    mu2 = F.conv2d(restored_img, window, padding=0, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(original_img ** 2, window, padding=0, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(restored_img ** 2, window, padding=0, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(original_img * restored_img, window, padding=0, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * max_pix_val) ** 2\n",
        "    C2 = (0.03 * max_pix_val) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "\n",
        "    return (((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)).mean()\n",
        "\n",
        "\n",
        "def compute_evaluation(test_dataloader, model, device='cpu'):\n",
        "  model.eval()\n",
        "  psnr_values = []\n",
        "  ssim_values = []\n",
        "\n",
        "  for _, (inputs, targets) in enumerate(test_dataloader):\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          predictions = model(inputs)\n",
        "\n",
        "      psnr_values.append(psnr(targets,predictions))\n",
        "      ssim_values.append(ssim(targets,predictions))\n",
        "\n",
        "  return torch.mean(torch.Tensor(psnr_values)).item(),torch.mean(torch.Tensor(ssim_values)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rzLgWkBY50K"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "Btr_ivEiY73R"
      },
      "outputs": [],
      "source": [
        "# Attention components\n",
        "# Attention module\n",
        "class W_MSA(nn.Module):\n",
        "  def __init__(self, dim=32, num_heads=8, qkv_bias=False):\n",
        "    super(W_MSA, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = dim // num_heads\n",
        "\n",
        "    # nn.Linear(in_features, out_features): the input of the layer has to have the last dimension equal to in_features (namely (*, in_features)). The output of the layer has the\n",
        "    # same dimension of the input except for the last one which is equal to out_features (namely (*, out_features))\n",
        "\n",
        "    # self.qkv = nn.Linear(dim, num_heads, self.head_dim, bias=qkv_bias) # this layer returns the queries, keys and values\n",
        "    self.qkv = nn.Linear(dim, self.head_dim*self.num_heads*3, bias=qkv_bias)\n",
        "\n",
        "    # these are default layers for the attention module\n",
        "    self.proj = nn.Linear(dim, dim)\n",
        "    self.proj_drop = nn.Dropout(0.)\n",
        "    self.attn_drop = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    # print(\"********* x shape: \",x.shape)\n",
        "    # print(\"********* self qkv shape:\",self.qkv(x).shape)\n",
        "\n",
        "    qkv_temp = self.qkv(x)\n",
        "    mult = qkv_temp.shape[0] * qkv_temp.shape[1] * qkv_temp.shape[2]\n",
        "    mult = mult//3\n",
        "    mult = mult//(self.num_heads*4)\n",
        "\n",
        "    if(B==global_var['batch_size']):\n",
        "      mult = mult//global_var['batch_size']\n",
        "    else:\n",
        "      mult = mult//global_var['eval_batch_size']\n",
        "\n",
        "    qkv = qkv_temp.reshape(B, mult, self.num_heads*4, 3)\n",
        "    # qkv = self.qkv(x).reshape(B, N, self.num_heads*3, C // self.num_heads).permute(2, 0, 3, 1)\n",
        "    # print(\"QKV: \",self.qkv(x).shape)\n",
        "    # print(\"QKV reshaped: \",self.qkv(x).reshape(B, mult, self.num_heads*4, 3).shape)\n",
        "\n",
        "    # print(\"********* qkv shape:\",qkv.shape)\n",
        "    q, k, v = qkv.unbind(dim=-1) # this returnes a tuple of tensors whose each element is portion of the original tensor (qkv) (ref: https://pytorch.org/docs/stable/generated/torch.unbind.html)\n",
        "\n",
        "    # this is the implementation of the attention formula described on the paper\n",
        "    scale = (C // self.head_dim) ** (0.5)\n",
        "    attn = ((q @ k.transpose(-2, -1)) // scale) + B # from the github: the final B can be also removed\n",
        "    attn = attn.softmax(dim=1)\n",
        "    attn = self.attn_drop(attn)\n",
        "    # print(\"********** x no reshape:\",(attn @ v).transpose(1, 2).shape)\n",
        "    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "\n",
        "    # these are default for the attention module\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# this is the simple implementation described in the paper\n",
        "class LeFF(nn.Module):\n",
        "  def __init__(self, dim=32, hidden_dim=128):\n",
        "    super(LeFF, self).__init__()\n",
        "    self.dim = dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.layer1 = nn.Sequential(nn.Linear(dim, hidden_dim), nn.GELU())\n",
        "    self.layer2 = nn.Sequential(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1), nn.GELU())\n",
        "    self.layer3 = nn.Sequential(nn.Linear(hidden_dim, dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(\"Before 1st layer x: \", x.shape)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x.permute(2,1,0))\n",
        "    x = self.layer3(x.permute(2,1,0))\n",
        "\n",
        "    return x\n",
        "\n",
        "  # def forward(self, x):\n",
        "  #   # bs x hw x c\n",
        "  #   bs, hw, c = x.size()\n",
        "  #   hh = int(math.sqrt(hw))\n",
        "\n",
        "  #   x = self.layer1(x)\n",
        "\n",
        "  #   # spatial restore\n",
        "  #   x = rearrange(x, ' b (h w) (c) -> b c h w ', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer2(x)\n",
        "\n",
        "  #   # flatten\n",
        "  #   x = rearrange(x, ' b c h w -> b (h w) c', h = hh, w = hh)\n",
        "\n",
        "  #   x = self.layer3(x)\n",
        "\n",
        "  #   return x\n",
        "\n",
        "# NN BLOCKS\n",
        "# LeWin Transformer Block (from the paper, it is made up of a sequence of: NormLayer, W_MSA, NormLayer, LeFF)\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(dim)\n",
        "    self.w_msa = W_MSA(dim=dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "    self.leff = LeFF(dim=dim)\n",
        "    self.dropout = nn.Dropout(0.)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dropout(self.norm1(x))\n",
        "    x = self.w_msa(x)\n",
        "    x = self.dropout(self.norm2(x))\n",
        "    x = self.leff(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Down-sampling Block (reduces the size of the feature map)\n",
        "# reshape the flattened features into 2D spatial feature maps, and then down-sample the maps, double the channels using 4 × 4 convolution with stride 2\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DownsampleBlock, self).__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # remember that x is a tensor!!\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        # print(\"*************** x: \",x.shape)\n",
        "        x = x.transpose(1, 2).contiguous().view(B, C, H, W) # this transposes the 1st and 2nd dimension of x, then the size of x is reshaped with view (the new size is (B, C, H, W))\n",
        "                                                            # (.contiguous() is required to make view workable, since view works only on contiguous data)\n",
        "\n",
        "        out = self.conv(x).flatten(2).transpose(1, 2).contiguous() # this pass the input x to the downsample layer, then the 2nd dimension of the output is flattened with the 3rd\n",
        "                                                                   # and finally its 1st and 2nd dimensions are transposed\n",
        "\n",
        "                                                                   # (B, C, H*W) is the size of the out after flatten(2)\n",
        "                                                                   # (B H*W C) is the final size of the out after transpose(1, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Up-sampling Block (reduces half of the channels and doubles the size of the feature map)\n",
        "# 2 × 2 transposed convolution with stride 2\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding):\n",
        "      super(UpsampleBlock, self).__init__()\n",
        "      self.in_channel = in_channel\n",
        "      self.out_channel = out_channel\n",
        "      self.deconv = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,padding=padding),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      B, L, C = x.shape\n",
        "      H = int(math.sqrt(L))\n",
        "      W = int(math.sqrt(L))\n",
        "      x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "      out = self.deconv(x).flatten(2).transpose(1, 2).contiguous() # B H*W C\n",
        "\n",
        "      return out\n",
        "\n",
        "# Input Projection Block (extracts the low-level features)\n",
        "# 3 x 3 convolutional layer with LeakyReLu\n",
        "class InputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=3, out_channel=32, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C\n",
        "\n",
        "        return x\n",
        "\n",
        "# Output Projection Block (returns the residual)\n",
        "# 3 x 3 convolutional layer\n",
        "class OutputProjBlock(nn.Module):\n",
        "    def __init__(self, in_channel=64, out_channel=3, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=kernel_size//2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x # the output of this block si called residual\n",
        "\n",
        "# complete uformer class\n",
        "class Uformer(nn.Module):\n",
        "  def __init__(self, embed_dim=32):\n",
        "    super(Uformer, self).__init__()\n",
        "\n",
        "    # encoder\n",
        "    self.input_proj = InputProjBlock()\n",
        "\n",
        "    self.tranformerblock_0 = TransformerBlock(embed_dim)\n",
        "    self.downsample_0 = DownsampleBlock(embed_dim, embed_dim*2)\n",
        "\n",
        "    self.tranformerblock_1 = TransformerBlock(embed_dim*2)\n",
        "    self.downsample_1 = DownsampleBlock(embed_dim*2, embed_dim*4)\n",
        "\n",
        "    self.tranformerblock_2 = TransformerBlock(embed_dim*4)\n",
        "    self.downsample_2 = DownsampleBlock(embed_dim*4, embed_dim*8)\n",
        "\n",
        "    self.tranformerblock_3 = TransformerBlock(embed_dim*8)\n",
        "    self.downsample_3 = DownsampleBlock(embed_dim*8, embed_dim*16)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    self.tranformerblock_4 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    self.upsample_0 = UpsampleBlock(embed_dim*16, embed_dim*8,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_5 = TransformerBlock(embed_dim*16)\n",
        "\n",
        "    self.upsample_1 = UpsampleBlock(embed_dim*16, embed_dim*4,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_6 = TransformerBlock(embed_dim*8)\n",
        "\n",
        "    self.upsample_2 = UpsampleBlock(embed_dim*8, embed_dim*2,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_7 = TransformerBlock(embed_dim*4)\n",
        "\n",
        "    self.upsample_3 = UpsampleBlock(embed_dim*4, embed_dim,kernel_size=2,stride=2,padding=0)\n",
        "    self.tranformerblock_8 = TransformerBlock(embed_dim*2)\n",
        "\n",
        "    self.output_proj = OutputProjBlock()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    degraded_image = x # x is the degraded image\n",
        "\n",
        "\n",
        "    # encoder\n",
        "    y = self.input_proj(x)\n",
        "    t0 = self.tranformerblock_0(y)\n",
        "    d0 = self.downsample_0(t0)\n",
        "    t1 = self.tranformerblock_1(d0)\n",
        "    d1 = self.downsample_1(t1)\n",
        "    t2 = self.tranformerblock_2(d1)\n",
        "    d2 = self.downsample_2(t2)\n",
        "    t3 = self.tranformerblock_3(d2)\n",
        "    d3 = self.downsample_3(t3)\n",
        "\n",
        "\n",
        "    # bottleneck\n",
        "    t4 = self.tranformerblock_4(d3)\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    u0 = self.upsample_0(t4)\n",
        "    # print(\"1) Upsampled in: \",u0.shape)\n",
        "    # print(\"1) Skipped in: \",t3.shape)\n",
        "    skippedconn_0 = torch.cat([u0, t3], -1) # this creates a skipped connection between t3 and t6 (u0 would have to be the input of t5)\n",
        "    t5 = self.tranformerblock_5(skippedconn_0)\n",
        "\n",
        "    u1 = self.upsample_1(t5)\n",
        "    # print(\"2) Upsampled in: \",u1.shape)\n",
        "    # print(\"2) Skipped in: \",t2.shape)\n",
        "    skippedconn_1 = torch.cat([u1, t2], -1)\n",
        "    t6 = self.tranformerblock_6(skippedconn_1)\n",
        "\n",
        "    u2 = self.upsample_2(t6)\n",
        "    # print(\"3) Upsampled in: \",u2.shape)\n",
        "    # print(\"3) Skipped in: \",t1.shape)\n",
        "    skippedconn_2 = torch.cat([u2, t1], -1)\n",
        "    t7 = self.tranformerblock_7(skippedconn_2)\n",
        "\n",
        "    u3 = self.upsample_3(t7)\n",
        "    # print(\"4) Upsampled in: \",u3.shape)\n",
        "    # print(\"4) Skipped in: \",t0.shape)\n",
        "    skippedconn_3 = torch.cat([u3, t0], -1)\n",
        "    t8 = self.tranformerblock_8(skippedconn_3)\n",
        "\n",
        "    residual = self.output_proj(t8)\n",
        "\n",
        "\n",
        "    # final residual summation\n",
        "    # print(\"Degraded: \",degraded_image.shape)\n",
        "    # print(\"Residual: \",residual.shape)\n",
        "    restored_image = degraded_image + residual.reshape([residual.shape[0],residual.shape[1],degraded_image.shape[2],degraded_image.shape[3]])\n",
        "\n",
        "    return restored_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCfDRBKpV_y"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "7IkSzCVHpZxp"
      },
      "outputs": [],
      "source": [
        "def train(device,train_dataloader,test_dataloader):\n",
        "  # Set-seed\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(global_var['seed'])\n",
        "  np.random.seed(global_var['seed'])\n",
        "  torch.cuda.manual_seed(global_var['seed'])\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Globals\n",
        "  history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': []}\n",
        "  min_acc = 0\n",
        "  train_acc_list = []\n",
        "  test_acc_list = []\n",
        "  train_loss_list = []\n",
        "  test_loss_list = []\n",
        "\n",
        "  # Loss\n",
        "  criterion = Cha_loss()\n",
        "\n",
        "  # Model\n",
        "  model = Uformer()\n",
        "  model.to(device=device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=[global_var['RGB_img_res'][0],global_var['patch_size'],global_var['patch_size']])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # Optimizer\n",
        "  optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=global_var['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
        "  )\n",
        "\n",
        "  # Scheduler\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=global_var['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
        "    cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
        "  )\n",
        "\n",
        "  # Early stopping\n",
        "  trigger_times, early_stopping_epochs = 0, global_var['e_stop_epochs']\n",
        "\n",
        "  print(\"--- Start training: {} ---\\n\".format(model_name))\n",
        "  # Train\n",
        "\n",
        "  for epoch in range(global_var['epochs']):\n",
        "    iter = 1\n",
        "    model.train(mode=True)\n",
        "    running_loss, accuracy = 0, 0\n",
        "\n",
        "    with tqdm(train_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Training\")\n",
        "\n",
        "        # Load data\n",
        "        inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "        # Forward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward\n",
        "        loss = torch.clone(loss).detach().requires_grad_(True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluation and Stats\n",
        "        running_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "\n",
        "        accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "        tepoch.set_postfix({'Loss': running_loss / iter,\n",
        "                            'Acc': accuracy.item() / iter,\n",
        "                            'Lr': global_var['lr'] if not history['lrs'] else history['lrs'][-1]})\n",
        "        iter += 1\n",
        "\n",
        "    # Validation\n",
        "    iter = 1\n",
        "    model.eval()\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    with tqdm(test_dataloader, unit=\"step\", position=0, leave=True) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch + 1}/{global_var['epochs']} - Validation\")\n",
        "        inputs, targets = batch[0].to(device=device), batch[1].to(device=device)\n",
        "\n",
        "        # Validation loop\n",
        "        with torch.no_grad():\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Evaluation metrics\n",
        "          test_accuracy += compute_accuracy(outputs, targets)\n",
        "\n",
        "          # Loss\n",
        "          loss = criterion(outputs, targets)\n",
        "          test_loss += loss.item()\n",
        "          test_loss_list.append(loss.item())\n",
        "\n",
        "          tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter})\n",
        "          iter += 1\n",
        "\n",
        "        # Update history infos\n",
        "        history['lrs'].append(get_lr(optimizer))\n",
        "        history['train_loss'].append(running_loss / len(train_dataloader))\n",
        "        history['val_loss'].append(test_loss / len(test_dataloader))\n",
        "        history['train_acc'].append(accuracy.item() / len(train_dataloader))\n",
        "        history['val_acc'].append(test_accuracy.item() / len(test_dataloader))\n",
        "\n",
        "        # Save model by best ACCURACY\n",
        "        if min_acc <= (test_accuracy / len(test_dataloader)):\n",
        "          min_acc = test_accuracy / len(test_dataloader)\n",
        "          save_checkpoint(model, model_name + '_best_acc')\n",
        "          print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
        "\n",
        "          if trigger_times > 4:\n",
        "            trigger_times = trigger_times - 2\n",
        "            print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
        "\n",
        "\n",
        "        save_history(history, save_model_root + model_name + '_history')\n",
        "        # Empty CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if trigger_times == early_stopping_epochs:\n",
        "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
        "            break\n",
        "\n",
        "        # Save loss for graphs\n",
        "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
        "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
        "\n",
        "  print('--- Finished Training ---')\n",
        "  # save_csv_history(model_name=model_name)\n",
        "  # plot_history(history)\n",
        "  # plot_loss(history, title='Loss Trend')\n",
        "\n",
        "  return history, min_acc, train_acc_list, test_acc_list, train_loss_list, test_loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQvL4hihzUer",
        "outputId": "e0dc3cd5-5595-4ade-ee74-235fdda51cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual device:  cuda:0\n",
            "========================================================================================================\n",
            "                                                   Kernel Shape  \\\n",
            "Layer                                                             \n",
            "0_input_proj.proj.Conv2d_0                        [3, 32, 3, 3]   \n",
            "1_input_proj.proj.LeakyReLU_1                                 -   \n",
            "2_tranformerblock_0.LayerNorm_norm1                        [32]   \n",
            "3_tranformerblock_0.Dropout_dropout                           -   \n",
            "4_tranformerblock_0.w_msa.Linear_qkv                   [32, 96]   \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop                   -   \n",
            "6_tranformerblock_0.w_msa.Linear_proj                  [32, 32]   \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop                   -   \n",
            "8_tranformerblock_0.LayerNorm_norm2                        [32]   \n",
            "9_tranformerblock_0.Dropout_dropout                           -   \n",
            "10_tranformerblock_0.leff.layer1.Linear_0             [32, 128]   \n",
            "11_tranformerblock_0.leff.layer1.GELU_1                       -   \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "13_tranformerblock_0.leff.layer2.GELU_1                       -   \n",
            "14_tranformerblock_0.leff.layer3.Linear_0             [128, 32]   \n",
            "15_downsample_0.conv.Conv2d_0                    [32, 64, 4, 4]   \n",
            "16_tranformerblock_1.LayerNorm_norm1                       [64]   \n",
            "17_tranformerblock_1.Dropout_dropout                          -   \n",
            "18_tranformerblock_1.w_msa.Linear_qkv                 [64, 192]   \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop                  -   \n",
            "20_tranformerblock_1.w_msa.Linear_proj                 [64, 64]   \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop                  -   \n",
            "22_tranformerblock_1.LayerNorm_norm2                       [64]   \n",
            "23_tranformerblock_1.Dropout_dropout                          -   \n",
            "24_tranformerblock_1.leff.layer1.Linear_0             [64, 128]   \n",
            "25_tranformerblock_1.leff.layer1.GELU_1                       -   \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "27_tranformerblock_1.leff.layer2.GELU_1                       -   \n",
            "28_tranformerblock_1.leff.layer3.Linear_0             [128, 64]   \n",
            "29_downsample_1.conv.Conv2d_0                   [64, 128, 4, 4]   \n",
            "30_tranformerblock_2.LayerNorm_norm1                      [128]   \n",
            "31_tranformerblock_2.Dropout_dropout                          -   \n",
            "32_tranformerblock_2.w_msa.Linear_qkv                [128, 384]   \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop                  -   \n",
            "34_tranformerblock_2.w_msa.Linear_proj               [128, 128]   \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop                  -   \n",
            "36_tranformerblock_2.LayerNorm_norm2                      [128]   \n",
            "37_tranformerblock_2.Dropout_dropout                          -   \n",
            "38_tranformerblock_2.leff.layer1.Linear_0            [128, 128]   \n",
            "39_tranformerblock_2.leff.layer1.GELU_1                       -   \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "41_tranformerblock_2.leff.layer2.GELU_1                       -   \n",
            "42_tranformerblock_2.leff.layer3.Linear_0            [128, 128]   \n",
            "43_downsample_2.conv.Conv2d_0                  [128, 256, 4, 4]   \n",
            "44_tranformerblock_3.LayerNorm_norm1                      [256]   \n",
            "45_tranformerblock_3.Dropout_dropout                          -   \n",
            "46_tranformerblock_3.w_msa.Linear_qkv                [256, 768]   \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop                  -   \n",
            "48_tranformerblock_3.w_msa.Linear_proj               [256, 256]   \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop                  -   \n",
            "50_tranformerblock_3.LayerNorm_norm2                      [256]   \n",
            "51_tranformerblock_3.Dropout_dropout                          -   \n",
            "52_tranformerblock_3.leff.layer1.Linear_0            [256, 128]   \n",
            "53_tranformerblock_3.leff.layer1.GELU_1                       -   \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "55_tranformerblock_3.leff.layer2.GELU_1                       -   \n",
            "56_tranformerblock_3.leff.layer3.Linear_0            [128, 256]   \n",
            "57_downsample_3.conv.Conv2d_0                  [256, 512, 4, 4]   \n",
            "58_tranformerblock_4.LayerNorm_norm1                      [512]   \n",
            "59_tranformerblock_4.Dropout_dropout                          -   \n",
            "60_tranformerblock_4.w_msa.Linear_qkv               [512, 1536]   \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop                  -   \n",
            "62_tranformerblock_4.w_msa.Linear_proj               [512, 512]   \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop                  -   \n",
            "64_tranformerblock_4.LayerNorm_norm2                      [512]   \n",
            "65_tranformerblock_4.Dropout_dropout                          -   \n",
            "66_tranformerblock_4.leff.layer1.Linear_0            [512, 128]   \n",
            "67_tranformerblock_4.leff.layer1.GELU_1                       -   \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "69_tranformerblock_4.leff.layer2.GELU_1                       -   \n",
            "70_tranformerblock_4.leff.layer3.Linear_0            [128, 512]   \n",
            "71_upsample_0.deconv.ConvTranspose2d_0         [256, 512, 2, 2]   \n",
            "72_tranformerblock_5.LayerNorm_norm1                      [512]   \n",
            "73_tranformerblock_5.Dropout_dropout                          -   \n",
            "74_tranformerblock_5.w_msa.Linear_qkv               [512, 1536]   \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop                  -   \n",
            "76_tranformerblock_5.w_msa.Linear_proj               [512, 512]   \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop                  -   \n",
            "78_tranformerblock_5.LayerNorm_norm2                      [512]   \n",
            "79_tranformerblock_5.Dropout_dropout                          -   \n",
            "80_tranformerblock_5.leff.layer1.Linear_0            [512, 128]   \n",
            "81_tranformerblock_5.leff.layer1.GELU_1                       -   \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "83_tranformerblock_5.leff.layer2.GELU_1                       -   \n",
            "84_tranformerblock_5.leff.layer3.Linear_0            [128, 512]   \n",
            "85_upsample_1.deconv.ConvTranspose2d_0         [128, 512, 2, 2]   \n",
            "86_tranformerblock_6.LayerNorm_norm1                      [256]   \n",
            "87_tranformerblock_6.Dropout_dropout                          -   \n",
            "88_tranformerblock_6.w_msa.Linear_qkv                [256, 768]   \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop                  -   \n",
            "90_tranformerblock_6.w_msa.Linear_proj               [256, 256]   \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop                  -   \n",
            "92_tranformerblock_6.LayerNorm_norm2                      [256]   \n",
            "93_tranformerblock_6.Dropout_dropout                          -   \n",
            "94_tranformerblock_6.leff.layer1.Linear_0            [256, 128]   \n",
            "95_tranformerblock_6.leff.layer1.GELU_1                       -   \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0      [128, 128, 3, 3]   \n",
            "97_tranformerblock_6.leff.layer2.GELU_1                       -   \n",
            "98_tranformerblock_6.leff.layer3.Linear_0            [128, 256]   \n",
            "99_upsample_2.deconv.ConvTranspose2d_0          [64, 256, 2, 2]   \n",
            "100_tranformerblock_7.LayerNorm_norm1                     [128]   \n",
            "101_tranformerblock_7.Dropout_dropout                         -   \n",
            "102_tranformerblock_7.w_msa.Linear_qkv               [128, 384]   \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop                 -   \n",
            "104_tranformerblock_7.w_msa.Linear_proj              [128, 128]   \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop                 -   \n",
            "106_tranformerblock_7.LayerNorm_norm2                     [128]   \n",
            "107_tranformerblock_7.Dropout_dropout                         -   \n",
            "108_tranformerblock_7.leff.layer1.Linear_0           [128, 128]   \n",
            "109_tranformerblock_7.leff.layer1.GELU_1                      -   \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0     [128, 128, 3, 3]   \n",
            "111_tranformerblock_7.leff.layer2.GELU_1                      -   \n",
            "112_tranformerblock_7.leff.layer3.Linear_0           [128, 128]   \n",
            "113_upsample_3.deconv.ConvTranspose2d_0         [32, 128, 2, 2]   \n",
            "114_tranformerblock_8.LayerNorm_norm1                      [64]   \n",
            "115_tranformerblock_8.Dropout_dropout                         -   \n",
            "116_tranformerblock_8.w_msa.Linear_qkv                [64, 192]   \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop                 -   \n",
            "118_tranformerblock_8.w_msa.Linear_proj                [64, 64]   \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop                 -   \n",
            "120_tranformerblock_8.LayerNorm_norm2                      [64]   \n",
            "121_tranformerblock_8.Dropout_dropout                         -   \n",
            "122_tranformerblock_8.leff.layer1.Linear_0            [64, 128]   \n",
            "123_tranformerblock_8.leff.layer1.GELU_1                      -   \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0     [128, 128, 3, 3]   \n",
            "125_tranformerblock_8.leff.layer2.GELU_1                      -   \n",
            "126_tranformerblock_8.leff.layer3.Linear_0            [128, 64]   \n",
            "127_output_proj.proj.Conv2d_0                     [64, 3, 3, 3]   \n",
            "128_output_proj.proj.LeakyReLU_1                              -   \n",
            "\n",
            "                                                   Output Shape     Params  \\\n",
            "Layer                                                                        \n",
            "0_input_proj.proj.Conv2d_0                     [32, 32, 32, 32]      896.0   \n",
            "1_input_proj.proj.LeakyReLU_1                  [32, 32, 32, 32]          -   \n",
            "2_tranformerblock_0.LayerNorm_norm1              [32, 1024, 32]       64.0   \n",
            "3_tranformerblock_0.Dropout_dropout              [32, 1024, 32]          -   \n",
            "4_tranformerblock_0.w_msa.Linear_qkv             [32, 1024, 96]     3.072k   \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop    [32, 1024, 1024]          -   \n",
            "6_tranformerblock_0.w_msa.Linear_proj            [32, 1024, 32]     1.056k   \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop      [32, 1024, 32]          -   \n",
            "8_tranformerblock_0.LayerNorm_norm2              [32, 1024, 32]       64.0   \n",
            "9_tranformerblock_0.Dropout_dropout              [32, 1024, 32]          -   \n",
            "10_tranformerblock_0.leff.layer1.Linear_0       [32, 1024, 128]     4.224k   \n",
            "11_tranformerblock_0.leff.layer1.GELU_1         [32, 1024, 128]          -   \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0       [128, 1024, 32]   147.584k   \n",
            "13_tranformerblock_0.leff.layer2.GELU_1         [128, 1024, 32]          -   \n",
            "14_tranformerblock_0.leff.layer3.Linear_0        [32, 1024, 32]     4.128k   \n",
            "15_downsample_0.conv.Conv2d_0                  [32, 64, 16, 16]    32.832k   \n",
            "16_tranformerblock_1.LayerNorm_norm1              [32, 256, 64]      128.0   \n",
            "17_tranformerblock_1.Dropout_dropout              [32, 256, 64]          -   \n",
            "18_tranformerblock_1.w_msa.Linear_qkv            [32, 256, 192]    12.288k   \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop     [32, 512, 512]          -   \n",
            "20_tranformerblock_1.w_msa.Linear_proj            [32, 256, 64]      4.16k   \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop      [32, 256, 64]          -   \n",
            "22_tranformerblock_1.LayerNorm_norm2              [32, 256, 64]      128.0   \n",
            "23_tranformerblock_1.Dropout_dropout              [32, 256, 64]          -   \n",
            "24_tranformerblock_1.leff.layer1.Linear_0        [32, 256, 128]      8.32k   \n",
            "25_tranformerblock_1.leff.layer1.GELU_1          [32, 256, 128]          -   \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0        [128, 256, 32]   147.584k   \n",
            "27_tranformerblock_1.leff.layer2.GELU_1          [128, 256, 32]          -   \n",
            "28_tranformerblock_1.leff.layer3.Linear_0         [32, 256, 64]     8.256k   \n",
            "29_downsample_1.conv.Conv2d_0                   [32, 128, 8, 8]     131.2k   \n",
            "30_tranformerblock_2.LayerNorm_norm1              [32, 64, 128]      256.0   \n",
            "31_tranformerblock_2.Dropout_dropout              [32, 64, 128]          -   \n",
            "32_tranformerblock_2.w_msa.Linear_qkv             [32, 64, 384]    49.152k   \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop     [32, 256, 256]          -   \n",
            "34_tranformerblock_2.w_msa.Linear_proj            [32, 64, 128]    16.512k   \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop      [32, 64, 128]          -   \n",
            "36_tranformerblock_2.LayerNorm_norm2              [32, 64, 128]      256.0   \n",
            "37_tranformerblock_2.Dropout_dropout              [32, 64, 128]          -   \n",
            "38_tranformerblock_2.leff.layer1.Linear_0         [32, 64, 128]    16.512k   \n",
            "39_tranformerblock_2.leff.layer1.GELU_1           [32, 64, 128]          -   \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0         [128, 64, 32]   147.584k   \n",
            "41_tranformerblock_2.leff.layer2.GELU_1           [128, 64, 32]          -   \n",
            "42_tranformerblock_2.leff.layer3.Linear_0         [32, 64, 128]    16.512k   \n",
            "43_downsample_2.conv.Conv2d_0                   [32, 256, 4, 4]   524.544k   \n",
            "44_tranformerblock_3.LayerNorm_norm1              [32, 16, 256]      512.0   \n",
            "45_tranformerblock_3.Dropout_dropout              [32, 16, 256]          -   \n",
            "46_tranformerblock_3.w_msa.Linear_qkv             [32, 16, 768]   196.608k   \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop     [32, 128, 128]          -   \n",
            "48_tranformerblock_3.w_msa.Linear_proj            [32, 16, 256]    65.792k   \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop      [32, 16, 256]          -   \n",
            "50_tranformerblock_3.LayerNorm_norm2              [32, 16, 256]      512.0   \n",
            "51_tranformerblock_3.Dropout_dropout              [32, 16, 256]          -   \n",
            "52_tranformerblock_3.leff.layer1.Linear_0         [32, 16, 128]    32.896k   \n",
            "53_tranformerblock_3.leff.layer1.GELU_1           [32, 16, 128]          -   \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0         [128, 16, 32]   147.584k   \n",
            "55_tranformerblock_3.leff.layer2.GELU_1           [128, 16, 32]          -   \n",
            "56_tranformerblock_3.leff.layer3.Linear_0         [32, 16, 256]    33.024k   \n",
            "57_downsample_3.conv.Conv2d_0                   [32, 512, 2, 2]  2.097664M   \n",
            "58_tranformerblock_4.LayerNorm_norm1               [32, 4, 512]     1.024k   \n",
            "59_tranformerblock_4.Dropout_dropout               [32, 4, 512]          -   \n",
            "60_tranformerblock_4.w_msa.Linear_qkv             [32, 4, 1536]   786.432k   \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop       [32, 64, 64]          -   \n",
            "62_tranformerblock_4.w_msa.Linear_proj             [32, 4, 512]   262.656k   \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop       [32, 4, 512]          -   \n",
            "64_tranformerblock_4.LayerNorm_norm2               [32, 4, 512]     1.024k   \n",
            "65_tranformerblock_4.Dropout_dropout               [32, 4, 512]          -   \n",
            "66_tranformerblock_4.leff.layer1.Linear_0          [32, 4, 128]    65.664k   \n",
            "67_tranformerblock_4.leff.layer1.GELU_1            [32, 4, 128]          -   \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0          [128, 4, 32]   147.584k   \n",
            "69_tranformerblock_4.leff.layer2.GELU_1            [128, 4, 32]          -   \n",
            "70_tranformerblock_4.leff.layer3.Linear_0          [32, 4, 512]    66.048k   \n",
            "71_upsample_0.deconv.ConvTranspose2d_0          [32, 256, 4, 4]   524.544k   \n",
            "72_tranformerblock_5.LayerNorm_norm1              [32, 16, 512]     1.024k   \n",
            "73_tranformerblock_5.Dropout_dropout              [32, 16, 512]          -   \n",
            "74_tranformerblock_5.w_msa.Linear_qkv            [32, 16, 1536]   786.432k   \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop     [32, 256, 256]          -   \n",
            "76_tranformerblock_5.w_msa.Linear_proj            [32, 16, 512]   262.656k   \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop      [32, 16, 512]          -   \n",
            "78_tranformerblock_5.LayerNorm_norm2              [32, 16, 512]     1.024k   \n",
            "79_tranformerblock_5.Dropout_dropout              [32, 16, 512]          -   \n",
            "80_tranformerblock_5.leff.layer1.Linear_0         [32, 16, 128]    65.664k   \n",
            "81_tranformerblock_5.leff.layer1.GELU_1           [32, 16, 128]          -   \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0         [128, 16, 32]   147.584k   \n",
            "83_tranformerblock_5.leff.layer2.GELU_1           [128, 16, 32]          -   \n",
            "84_tranformerblock_5.leff.layer3.Linear_0         [32, 16, 512]    66.048k   \n",
            "85_upsample_1.deconv.ConvTranspose2d_0          [32, 128, 8, 8]   262.272k   \n",
            "86_tranformerblock_6.LayerNorm_norm1              [32, 64, 256]      512.0   \n",
            "87_tranformerblock_6.Dropout_dropout              [32, 64, 256]          -   \n",
            "88_tranformerblock_6.w_msa.Linear_qkv             [32, 64, 768]   196.608k   \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop     [32, 512, 512]          -   \n",
            "90_tranformerblock_6.w_msa.Linear_proj            [32, 64, 256]    65.792k   \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop      [32, 64, 256]          -   \n",
            "92_tranformerblock_6.LayerNorm_norm2              [32, 64, 256]      512.0   \n",
            "93_tranformerblock_6.Dropout_dropout              [32, 64, 256]          -   \n",
            "94_tranformerblock_6.leff.layer1.Linear_0         [32, 64, 128]    32.896k   \n",
            "95_tranformerblock_6.leff.layer1.GELU_1           [32, 64, 128]          -   \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0         [128, 64, 32]   147.584k   \n",
            "97_tranformerblock_6.leff.layer2.GELU_1           [128, 64, 32]          -   \n",
            "98_tranformerblock_6.leff.layer3.Linear_0         [32, 64, 256]    33.024k   \n",
            "99_upsample_2.deconv.ConvTranspose2d_0         [32, 64, 16, 16]      65.6k   \n",
            "100_tranformerblock_7.LayerNorm_norm1            [32, 256, 128]      256.0   \n",
            "101_tranformerblock_7.Dropout_dropout            [32, 256, 128]          -   \n",
            "102_tranformerblock_7.w_msa.Linear_qkv           [32, 256, 384]    49.152k   \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop  [32, 1024, 1024]          -   \n",
            "104_tranformerblock_7.w_msa.Linear_proj          [32, 256, 128]    16.512k   \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop    [32, 256, 128]          -   \n",
            "106_tranformerblock_7.LayerNorm_norm2            [32, 256, 128]      256.0   \n",
            "107_tranformerblock_7.Dropout_dropout            [32, 256, 128]          -   \n",
            "108_tranformerblock_7.leff.layer1.Linear_0       [32, 256, 128]    16.512k   \n",
            "109_tranformerblock_7.leff.layer1.GELU_1         [32, 256, 128]          -   \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0       [128, 256, 32]   147.584k   \n",
            "111_tranformerblock_7.leff.layer2.GELU_1         [128, 256, 32]          -   \n",
            "112_tranformerblock_7.leff.layer3.Linear_0       [32, 256, 128]    16.512k   \n",
            "113_upsample_3.deconv.ConvTranspose2d_0        [32, 32, 32, 32]    16.416k   \n",
            "114_tranformerblock_8.LayerNorm_norm1            [32, 1024, 64]      128.0   \n",
            "115_tranformerblock_8.Dropout_dropout            [32, 1024, 64]          -   \n",
            "116_tranformerblock_8.w_msa.Linear_qkv          [32, 1024, 192]    12.288k   \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop  [32, 2048, 2048]          -   \n",
            "118_tranformerblock_8.w_msa.Linear_proj          [32, 1024, 64]      4.16k   \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop    [32, 1024, 64]          -   \n",
            "120_tranformerblock_8.LayerNorm_norm2            [32, 1024, 64]      128.0   \n",
            "121_tranformerblock_8.Dropout_dropout            [32, 1024, 64]          -   \n",
            "122_tranformerblock_8.leff.layer1.Linear_0      [32, 1024, 128]      8.32k   \n",
            "123_tranformerblock_8.leff.layer1.GELU_1        [32, 1024, 128]          -   \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0      [128, 1024, 32]   147.584k   \n",
            "125_tranformerblock_8.leff.layer2.GELU_1        [128, 1024, 32]          -   \n",
            "126_tranformerblock_8.leff.layer3.Linear_0       [32, 1024, 64]     8.256k   \n",
            "127_output_proj.proj.Conv2d_0                   [32, 3, 32, 32]     1.731k   \n",
            "128_output_proj.proj.LeakyReLU_1                [32, 3, 32, 32]          -   \n",
            "\n",
            "                                                Mult-Adds  \n",
            "Layer                                                      \n",
            "0_input_proj.proj.Conv2d_0                       884.736k  \n",
            "1_input_proj.proj.LeakyReLU_1                           -  \n",
            "2_tranformerblock_0.LayerNorm_norm1                  32.0  \n",
            "3_tranformerblock_0.Dropout_dropout                     -  \n",
            "4_tranformerblock_0.w_msa.Linear_qkv               3.072k  \n",
            "5_tranformerblock_0.w_msa.Dropout_attn_drop             -  \n",
            "6_tranformerblock_0.w_msa.Linear_proj              1.024k  \n",
            "7_tranformerblock_0.w_msa.Dropout_proj_drop             -  \n",
            "8_tranformerblock_0.LayerNorm_norm2                  32.0  \n",
            "9_tranformerblock_0.Dropout_dropout                     -  \n",
            "10_tranformerblock_0.leff.layer1.Linear_0          4.096k  \n",
            "11_tranformerblock_0.leff.layer1.GELU_1                 -  \n",
            "12_tranformerblock_0.leff.layer2.Conv2d_0       4.718592M  \n",
            "13_tranformerblock_0.leff.layer2.GELU_1                 -  \n",
            "14_tranformerblock_0.leff.layer3.Linear_0          4.096k  \n",
            "15_downsample_0.conv.Conv2d_0                   8.388608M  \n",
            "16_tranformerblock_1.LayerNorm_norm1                 64.0  \n",
            "17_tranformerblock_1.Dropout_dropout                    -  \n",
            "18_tranformerblock_1.w_msa.Linear_qkv             12.288k  \n",
            "19_tranformerblock_1.w_msa.Dropout_attn_drop            -  \n",
            "20_tranformerblock_1.w_msa.Linear_proj             4.096k  \n",
            "21_tranformerblock_1.w_msa.Dropout_proj_drop            -  \n",
            "22_tranformerblock_1.LayerNorm_norm2                 64.0  \n",
            "23_tranformerblock_1.Dropout_dropout                    -  \n",
            "24_tranformerblock_1.leff.layer1.Linear_0          8.192k  \n",
            "25_tranformerblock_1.leff.layer1.GELU_1                 -  \n",
            "26_tranformerblock_1.leff.layer2.Conv2d_0       4.718592M  \n",
            "27_tranformerblock_1.leff.layer2.GELU_1                 -  \n",
            "28_tranformerblock_1.leff.layer3.Linear_0          8.192k  \n",
            "29_downsample_1.conv.Conv2d_0                   8.388608M  \n",
            "30_tranformerblock_2.LayerNorm_norm1                128.0  \n",
            "31_tranformerblock_2.Dropout_dropout                    -  \n",
            "32_tranformerblock_2.w_msa.Linear_qkv             49.152k  \n",
            "33_tranformerblock_2.w_msa.Dropout_attn_drop            -  \n",
            "34_tranformerblock_2.w_msa.Linear_proj            16.384k  \n",
            "35_tranformerblock_2.w_msa.Dropout_proj_drop            -  \n",
            "36_tranformerblock_2.LayerNorm_norm2                128.0  \n",
            "37_tranformerblock_2.Dropout_dropout                    -  \n",
            "38_tranformerblock_2.leff.layer1.Linear_0         16.384k  \n",
            "39_tranformerblock_2.leff.layer1.GELU_1                 -  \n",
            "40_tranformerblock_2.leff.layer2.Conv2d_0       4.718592M  \n",
            "41_tranformerblock_2.leff.layer2.GELU_1                 -  \n",
            "42_tranformerblock_2.leff.layer3.Linear_0         16.384k  \n",
            "43_downsample_2.conv.Conv2d_0                   8.388608M  \n",
            "44_tranformerblock_3.LayerNorm_norm1                256.0  \n",
            "45_tranformerblock_3.Dropout_dropout                    -  \n",
            "46_tranformerblock_3.w_msa.Linear_qkv            196.608k  \n",
            "47_tranformerblock_3.w_msa.Dropout_attn_drop            -  \n",
            "48_tranformerblock_3.w_msa.Linear_proj            65.536k  \n",
            "49_tranformerblock_3.w_msa.Dropout_proj_drop            -  \n",
            "50_tranformerblock_3.LayerNorm_norm2                256.0  \n",
            "51_tranformerblock_3.Dropout_dropout                    -  \n",
            "52_tranformerblock_3.leff.layer1.Linear_0         32.768k  \n",
            "53_tranformerblock_3.leff.layer1.GELU_1                 -  \n",
            "54_tranformerblock_3.leff.layer2.Conv2d_0       4.718592M  \n",
            "55_tranformerblock_3.leff.layer2.GELU_1                 -  \n",
            "56_tranformerblock_3.leff.layer3.Linear_0         32.768k  \n",
            "57_downsample_3.conv.Conv2d_0                   8.388608M  \n",
            "58_tranformerblock_4.LayerNorm_norm1                512.0  \n",
            "59_tranformerblock_4.Dropout_dropout                    -  \n",
            "60_tranformerblock_4.w_msa.Linear_qkv            786.432k  \n",
            "61_tranformerblock_4.w_msa.Dropout_attn_drop            -  \n",
            "62_tranformerblock_4.w_msa.Linear_proj           262.144k  \n",
            "63_tranformerblock_4.w_msa.Dropout_proj_drop            -  \n",
            "64_tranformerblock_4.LayerNorm_norm2                512.0  \n",
            "65_tranformerblock_4.Dropout_dropout                    -  \n",
            "66_tranformerblock_4.leff.layer1.Linear_0         65.536k  \n",
            "67_tranformerblock_4.leff.layer1.GELU_1                 -  \n",
            "68_tranformerblock_4.leff.layer2.Conv2d_0       4.718592M  \n",
            "69_tranformerblock_4.leff.layer2.GELU_1                 -  \n",
            "70_tranformerblock_4.leff.layer3.Linear_0         65.536k  \n",
            "71_upsample_0.deconv.ConvTranspose2d_0          8.388608M  \n",
            "72_tranformerblock_5.LayerNorm_norm1                512.0  \n",
            "73_tranformerblock_5.Dropout_dropout                    -  \n",
            "74_tranformerblock_5.w_msa.Linear_qkv            786.432k  \n",
            "75_tranformerblock_5.w_msa.Dropout_attn_drop            -  \n",
            "76_tranformerblock_5.w_msa.Linear_proj           262.144k  \n",
            "77_tranformerblock_5.w_msa.Dropout_proj_drop            -  \n",
            "78_tranformerblock_5.LayerNorm_norm2                512.0  \n",
            "79_tranformerblock_5.Dropout_dropout                    -  \n",
            "80_tranformerblock_5.leff.layer1.Linear_0         65.536k  \n",
            "81_tranformerblock_5.leff.layer1.GELU_1                 -  \n",
            "82_tranformerblock_5.leff.layer2.Conv2d_0       4.718592M  \n",
            "83_tranformerblock_5.leff.layer2.GELU_1                 -  \n",
            "84_tranformerblock_5.leff.layer3.Linear_0         65.536k  \n",
            "85_upsample_1.deconv.ConvTranspose2d_0         16.777216M  \n",
            "86_tranformerblock_6.LayerNorm_norm1                256.0  \n",
            "87_tranformerblock_6.Dropout_dropout                    -  \n",
            "88_tranformerblock_6.w_msa.Linear_qkv            196.608k  \n",
            "89_tranformerblock_6.w_msa.Dropout_attn_drop            -  \n",
            "90_tranformerblock_6.w_msa.Linear_proj            65.536k  \n",
            "91_tranformerblock_6.w_msa.Dropout_proj_drop            -  \n",
            "92_tranformerblock_6.LayerNorm_norm2                256.0  \n",
            "93_tranformerblock_6.Dropout_dropout                    -  \n",
            "94_tranformerblock_6.leff.layer1.Linear_0         32.768k  \n",
            "95_tranformerblock_6.leff.layer1.GELU_1                 -  \n",
            "96_tranformerblock_6.leff.layer2.Conv2d_0       4.718592M  \n",
            "97_tranformerblock_6.leff.layer2.GELU_1                 -  \n",
            "98_tranformerblock_6.leff.layer3.Linear_0         32.768k  \n",
            "99_upsample_2.deconv.ConvTranspose2d_0         16.777216M  \n",
            "100_tranformerblock_7.LayerNorm_norm1               128.0  \n",
            "101_tranformerblock_7.Dropout_dropout                   -  \n",
            "102_tranformerblock_7.w_msa.Linear_qkv            49.152k  \n",
            "103_tranformerblock_7.w_msa.Dropout_attn_drop           -  \n",
            "104_tranformerblock_7.w_msa.Linear_proj           16.384k  \n",
            "105_tranformerblock_7.w_msa.Dropout_proj_drop           -  \n",
            "106_tranformerblock_7.LayerNorm_norm2               128.0  \n",
            "107_tranformerblock_7.Dropout_dropout                   -  \n",
            "108_tranformerblock_7.leff.layer1.Linear_0        16.384k  \n",
            "109_tranformerblock_7.leff.layer1.GELU_1                -  \n",
            "110_tranformerblock_7.leff.layer2.Conv2d_0      4.718592M  \n",
            "111_tranformerblock_7.leff.layer2.GELU_1                -  \n",
            "112_tranformerblock_7.leff.layer3.Linear_0        16.384k  \n",
            "113_upsample_3.deconv.ConvTranspose2d_0        16.777216M  \n",
            "114_tranformerblock_8.LayerNorm_norm1                64.0  \n",
            "115_tranformerblock_8.Dropout_dropout                   -  \n",
            "116_tranformerblock_8.w_msa.Linear_qkv            12.288k  \n",
            "117_tranformerblock_8.w_msa.Dropout_attn_drop           -  \n",
            "118_tranformerblock_8.w_msa.Linear_proj            4.096k  \n",
            "119_tranformerblock_8.w_msa.Dropout_proj_drop           -  \n",
            "120_tranformerblock_8.LayerNorm_norm2                64.0  \n",
            "121_tranformerblock_8.Dropout_dropout                   -  \n",
            "122_tranformerblock_8.leff.layer1.Linear_0         8.192k  \n",
            "123_tranformerblock_8.leff.layer1.GELU_1                -  \n",
            "124_tranformerblock_8.leff.layer2.Conv2d_0      4.718592M  \n",
            "125_tranformerblock_8.leff.layer2.GELU_1                -  \n",
            "126_tranformerblock_8.leff.layer3.Linear_0         8.192k  \n",
            "127_output_proj.proj.Conv2d_0                   1.769472M  \n",
            "128_output_proj.proj.LeakyReLU_1                        -  \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params            8.287907M\n",
            "Trainable params        8.287907M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             140.689216M\n",
            "========================================================================================================\n",
            "The Uformer model has: 8287907 trainable parameters\n",
            "--- Start training: Uformer ---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Training: 100%|██████████| 4/4 [00:59<00:00, 14.90s/step, Loss=9.72, Acc=34.3, Lr=0.0002]\n",
            "Epoch 1/20 - Validation:   3%|▎         | 1/32 [00:01<00:36,  1.18s/step, Loss=11.4, Acc=49.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 2.533 at epoch 1\n",
            "New best ACCURACY: 3.070 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:   9%|▉         | 3/32 [00:02<00:19,  1.46step/s, Loss=10.4, Acc=48.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 4.558 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  12%|█▎        | 4/32 [00:02<00:13,  2.08step/s, Loss=9.32, Acc=41.3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 5.167 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  16%|█▌        | 5/32 [00:02<00:12,  2.12step/s, Loss=9.87, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 5.556 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20 - Validation:  19%|█▉        | 6/32 [00:03<00:10,  2.42step/s, Loss=9.87, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 5.716 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  19%|█▉        | 6/32 [00:03<00:10,  2.42step/s, Loss=9.3, Acc=31.5] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 6.885 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  25%|██▌       | 8/32 [00:04<00:12,  1.94step/s, Loss=8.7, Acc=31.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 7.855 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  28%|██▊       | 9/32 [00:05<00:12,  1.89step/s, Loss=8.73, Acc=29]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 8.147 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  28%|██▊       | 9/32 [00:05<00:12,  1.89step/s, Loss=9.1, Acc=26.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 8.324 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  34%|███▍      | 11/32 [00:06<00:11,  1.88step/s, Loss=10.9, Acc=26.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 8.511 at epoch 1\n",
            "New best ACCURACY: 9.789 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  41%|████      | 13/32 [00:07<00:09,  2.00step/s, Loss=10.5, Acc=24.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 10.091 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  44%|████▍     | 14/32 [00:07<00:09,  1.82step/s, Loss=10.4, Acc=25.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 10.989 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  47%|████▋     | 15/32 [00:08<00:08,  1.95step/s, Loss=10, Acc=28]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 13.565 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20 - Validation:  50%|█████     | 16/32 [00:08<00:06,  2.30step/s, Loss=10, Acc=28]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 13.982 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  50%|█████     | 16/32 [00:08<00:06,  2.30step/s, Loss=9.86, Acc=26.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 14.110 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  56%|█████▋    | 18/32 [00:09<00:05,  2.39step/s, Loss=9.69, Acc=28]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 15.768 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  59%|█████▉    | 19/32 [00:09<00:05,  2.25step/s, Loss=9.77, Acc=27.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 16.634 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20 - Validation:  62%|██████▎   | 20/32 [00:10<00:04,  2.60step/s, Loss=9.77, Acc=27.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 16.997 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  62%|██████▎   | 20/32 [00:10<00:04,  2.60step/s, Loss=9.58, Acc=29]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 19.057 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  69%|██████▉   | 22/32 [00:11<00:04,  2.19step/s, Loss=9.48, Acc=29]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 19.928 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  72%|███████▏  | 23/32 [00:11<00:04,  1.98step/s, Loss=9.17, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 20.147 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20 - Validation:  75%|███████▌  | 24/32 [00:12<00:03,  2.30step/s, Loss=9.17, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 22.892 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  78%|███████▊  | 25/32 [00:12<00:03,  2.08step/s, Loss=9.05, Acc=30.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 24.156 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  81%|████████▏ | 26/32 [00:13<00:03,  1.94step/s, Loss=9.18, Acc=31.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 25.630 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  84%|████████▍ | 27/32 [00:13<00:02,  2.15step/s, Loss=9.13, Acc=31.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 26.266 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  88%|████████▊ | 28/32 [00:14<00:02,  1.86step/s, Loss=9.25, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 26.689 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  91%|█████████ | 29/32 [00:14<00:01,  2.05step/s, Loss=9.22, Acc=30.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 27.636 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  94%|█████████▍| 30/32 [00:15<00:00,  2.13step/s, Loss=9.21, Acc=29.6]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 27.750 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation:  97%|█████████▋| 31/32 [00:15<00:00,  2.10step/s, Loss=8.97, Acc=30.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 30.296 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 - Validation: 100%|██████████| 32/32 [00:16<00:00,  1.97step/s, Loss=8.97, Acc=30.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New best ACCURACY: 30.841 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 - Training: 100%|██████████| 4/4 [00:58<00:00, 14.62s/step, Loss=9.83, Acc=36.3, Lr=0.0002]\n",
            "Epoch 2/20 - Validation: 100%|██████████| 32/32 [00:15<00:00,  2.11step/s, Loss=9.1, Acc=28.5]\n",
            "Epoch 3/20 - Training: 100%|██████████| 4/4 [00:59<00:00, 14.87s/step, Loss=9.54, Acc=34.7, Lr=0.0002]\n",
            "Epoch 3/20 - Validation: 100%|██████████| 32/32 [00:14<00:00,  2.19step/s, Loss=9.61, Acc=25.8]\n",
            "Epoch 4/20 - Training: 100%|██████████| 4/4 [00:59<00:00, 14.80s/step, Loss=9.7, Acc=34.3, Lr=0.0002]\n",
            "Epoch 4/20 - Validation: 100%|██████████| 32/32 [00:14<00:00,  2.22step/s, Loss=8.92, Acc=29.2]\n",
            "Epoch 5/20 - Training:  50%|█████     | 2/4 [00:38<00:38, 19.06s/step, Loss=8.81, Acc=33.9, Lr=0.0002]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-395-037ae5f01707>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhardware_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-394-a147a2a2e586>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, train_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mtepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{global_var['epochs']} - Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = hardware_check()\n",
        "stats = train(device,train_loader,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_model_root+\"stats.pkl\", 'rb') as f:\n",
        "  data = pickle.load(f)"
      ],
      "metadata": {
        "id": "kuOgkSq-JVHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = data[4]\n",
        "test_losses = data[5]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(25,5))\n",
        "\n",
        "ax.plot(train_losses)\n",
        "print(len(train_losses))\n",
        "# print(len(test_losses))\n",
        "# train_stats = data[0]\n",
        "# train_stats.keys()\n",
        "\n",
        "# train_stats['train_loss']\n",
        "\n",
        "# fig, ax = plt.subplots(4, 1, figsize=(5,5))\n",
        "# epochs = torch.linspace(0,global_var['epochs'],steps=960)\n",
        "\n",
        "# ax[0].set_title(\"Train loss\")\n",
        "# ax[0].plot(epochs,train_stats['train_loss'], color='orange')\n",
        "# ax[0].set_ylabel('Loss')\n",
        "# ax[0].set_xlabel('Epochs')\n",
        "\n",
        "# ax[1].set_title(\"Test loss\")\n",
        "# ax[1].plot(epochs,train_stats['val_loss'], color='red')\n",
        "# ax[1].set_ylabel('Loss')\n",
        "# ax[1].set_xlabel('Epochs')\n",
        "\n",
        "# ax[2].set_title(\"Train accuracy\")\n",
        "# ax[2].plot(epochs,train_stats['train_acc'], color='blue')\n",
        "# ax[2].set_ylabel('Loss')\n",
        "# ax[2].set_xlabel('Epochs')\n",
        "\n",
        "# ax[3].set_title(\"Test accuracy\")\n",
        "# ax[2].plot(epochs,train_stats['val_acc'], color='green')\n",
        "# ax[3].set_ylabel('Loss')\n",
        "# ax[3].set_xlabel('Epochs')\n",
        "\n",
        "# fig.tight_layout()"
      ],
      "metadata": {
        "id": "bq3N-KbJbVdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV0TTJ-b34SV"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkN_h9E935rb"
      },
      "outputs": [],
      "source": [
        "def test(device,test_dataloader):\n",
        "  model = Uformer()\n",
        "  model = load_pretrained_model(model,device)\n",
        "  model.to(device)\n",
        "\n",
        "  if global_var['do_print_model']:\n",
        "    print_model(model, device, input_shape=global_var['RGB_img_res'])\n",
        "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
        "\n",
        "  # Evaluate\n",
        "  print(\" --- Begin evaluation --- \")\n",
        "  mean_psnr, mean_ssim = compute_evaluation(test_dataloader,model,device)\n",
        "  print(\" --- End evaluation --- \")\n",
        "  print(\"Mean PSNR: \",mean_psnr)\n",
        "  print(\"Mean SSIM: \",mean_ssim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9TdNFPC6x14"
      },
      "outputs": [],
      "source": [
        "test(device,test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5KVlRJ0lu7eU",
        "smh0pVrvtHTZ",
        "maczFOi8eB52",
        "lV0TTJ-b34SV"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}